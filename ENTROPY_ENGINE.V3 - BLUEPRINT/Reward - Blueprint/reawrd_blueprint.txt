1. The "Curriculum Decay" Approach (OpenAI Style) ğŸ“‰
PouÅ¾ito napÅ™. pÅ™i trÃ©novÃ¡nÃ­ Dota 2 botÅ¯ nebo robotickÃ© ruky.

MyÅ¡lenka:
Na zaÄÃ¡tku trÃ©ninku potÅ™ebuje agent hustÃ© odmÄ›ny (Dense Rewards) - "Jsi blÃ­Å¾", "JedeÅ¡ rychle", "NedotÃ½kÃ¡Å¡ se".
Ale v produkci (ke konci trÃ©ninku) chceÅ¡, aby ho zajÃ­mal jen vÃ½sledek (Sparse Reward) - "VyhrÃ¡l jsi?", "Donesl jsi to?".

Pokud nechÃ¡Å¡ hustÃ© odmÄ›ny navÅ¾dy, agent bude tanÄit podivnÃ© tance, jen aby maximalizoval pomocnÃ© body, mÃ­sto aby Å¡el rovnou do cÃ­le.

Å˜eÅ¡enÃ­:
VytvoÅ™ promÄ›nnou curriculum_factor (od 1.0 do 0.0), kterÃ¡ se mÄ›nÃ­ bÄ›hem total_epochs.

python
# V trÃ©ninkovÃ© smyÄce:
progress = epoch / total_epochs  # 0.0 -> 1.0
alpha = 1.0 - progress           # 1.0 -> 0.0 (VÃ¡ha pro "pomocnÃ¡ koleÄka")

# V reward funkci:
reward = (
    w_reach * goal_reached +                  # 1. CÃL (VÅ¾dy platÃ­ 100%)
    alpha * (w_dist * dist_improvement) +     # 2. VZDÃLENOST (PostupnÄ› mizÃ­)
    alpha * (w_formation * formation_score)   # 3. FORMACE (PostupnÄ› mizÃ­, zbyde jen zvyk)
)
VÃ½sledek: Agent se na zaÄÃ¡tku nauÄÃ­ techniku, ale na konci uÅ¾ jede ÄistÄ› na vÃ½kon. "PomocnÃ¡ koleÄka" se za jÃ­zdy odmontujÃ­.

2. Potential-Based Reward Shaping (PBRS) (The Theoretical Gold Standard) ğŸŒŸ
PouÅ¾Ã­vÃ¡ se v akademickÃ©m vÃ½zkumu a u kritickÃ½ch systÃ©mÅ¯ (Berkeley, Stanford).

ProblÃ©m:
KdyÅ¾ odmÄ›ÅˆujeÅ¡ agenta za to, Å¾e je "blÃ­zko cÃ­le", mÅ¯Å¾e se stÃ¡t, Å¾e najde mÃ­sto, kde je blÃ­zko, a tam se zastavÃ­, protoÅ¾e pohyb dÃ¡l by mohl znamenat doÄasnÃ© zhorÅ¡enÃ­.

Å˜eÅ¡enÃ­ (Ng et al. 1999):
MÃ­sto odmÄ›ny za stav ($s$) dÃ¡vej odmÄ›nu za zmÄ›nu potenciÃ¡lu ($\Phi$).
Matematicky: $R = F(s, s') = \gamma \Phi(s') - \Phi(s)$

Lidsky: NeodmÄ›Åˆuj ho za to, Å¾e je blÃ­zko. OdmÄ›Åˆuj ho jen za to, Å¾e se pÅ™iblÃ­Å¾il vÃ­c neÅ¾ v minulÃ©m kroku.

python
# V kÃ³du:
# phi_current = -dist_to_goal
# phi_prev = -prev_dist_to_goal

# Å patnÄ› (StatickÃ¡ odmÄ›na):
# reward += phi_current 

# SprÃ¡vnÄ› (PBRS):
reward += gamma * phi_current - phi_prev
VÃ½sledek: Tohle matematicky zaruÄuje, Å¾e agent nebude cyklit (krouÅ¾it). Pokud se zastavÃ­, odmÄ›na je 0. MusÃ­ se hÃ½bat vpÅ™ed, aby dostal plusovÃ© body.

3. Action Regularization & Energy Cost (NVIDIA Isaac Gym / Robotics) ğŸ¤–
Tohle pouÅ¾Ã­vÃ¡ Boston Dynamics a NVIDIA pro roboty.

MyÅ¡lenka:
V simulaci je snadnÃ© mÄ›nit rychlost z 0 na 100 za jeden frame. V realitÄ› to zniÄÃ­ motory. "UÅ¡ubranÃ½" pohyb (Jerk) je znÃ¡mka Å¡patnÃ© policy.

Å˜eÅ¡enÃ­:
BrutÃ¡lnÄ› penalizuj nejen energii, ale i zmÄ›nu akce a zmÄ›nu zrychlenÃ­ (Jerk).

python
# Penalizace za "trhÃ¡nÃ­ volantem"
# action je vektor motorÅ¯ [-1, 1]
action_diff = jnp.sum(jnp.square(current_action - prev_action))
reward -= 0.1 * action_diff

# Penalizace za "prÃ¡ci" (Torque)
energy_cost = jnp.sum(jnp.square(current_action))
reward -= 0.01 * energy_cost
VÃ½sledek: Vznikne Smooth Control. Agenti se budou pohybovat plynule, pÅ™edvÃ­datelnÄ› a organicky. Nebudou vypadat jako glitchujÃ­cÃ­ pixely.

ğŸ”¥ FINÃLNÃ "PRODUCTION READY" REWARD FUNKCE (Copy-Paste)
Tady je kompilace toho nejlepÅ¡Ã­ho do jednÃ© JAX funkce.

python
def production_ready_reward(state, params, config, progress):
    """
    state: aktuÃ¡lnÃ­ stav
    prev_state: stav v minulÃ©m kroku
    progress: 0.0 (zaÄÃ¡tek) aÅ¾ 1.0 (konec trÃ©ninku)
    """
    
    # 1. SPARSE REWARD (To jedinÃ©, na Äem nakonec zÃ¡leÅ¾Ã­)
    # ObrovskÃ½ bonus za splnÄ›nÃ­ mise.
    is_at_target = jnp.linalg.norm(state.pos - state.target) < params.target_radius
    sparse_reward = is_at_target * 50.0 # VelkÃ¡ odmÄ›na
    
    # 2. SHAPING (PBRS) - NavÃ¡dÄ›nÃ­
    # VÅ¡imni si: PorovnÃ¡vÃ¡me aktuÃ¡lnÃ­ a minulou vzdÃ¡lenost (zlepÅ¡enÃ­)
    dist_curr = jnp.linalg.norm(state.pos - state.target)
    dist_prev = jnp.linalg.norm(state.prev_pos - state.target)
    shaping_reward = (config.gamma * (-dist_curr) - (-dist_prev)) * config.w_dist
    
    # 3. REGULARIZATION (Smoothness & Energy)
    # Tohle platÃ­ vÅ¾dy, chceme plynulÃ½ pohyb celou dobu
    action_diff = jnp.sum(jnp.square(state.action - state.prev_action))
    torque = jnp.sum(jnp.square(state.action))
    reg_penalty = (0.1 * action_diff) + (0.01 * torque)
    
    # 4. SAFETY & FORMATION (Curriculum)
    # PostupnÄ› sniÅ¾ujeme vÃ¡hu pomocnÃ½ch koleÄek (alpha)
    # Na zaÄÃ¡tku (progress=0) je alpha 1.0, na konci (progress=1) je 0.2 (stÃ¡le trochu chceme formaci)
    alpha = 1.0 - (0.8 * progress) 
    
    # Formace a kolize
    # ... (zde tvÅ¯j vÃ½poÄet formace) ...
    aux_reward = alpha * (formation_score + collision_penalty)

    # 5. EXISTENTIAL PENALTY (Time Pressure)
    # Motivuje to splnit Ãºkol rychle.
    time_penalty = -0.01

    # FINÃLNÃ SOUÄŒET
    total_reward = (
        sparse_reward + 
        shaping_reward + 
        aux_reward - 
        reg_penalty + 
        time_penalty
    )
    
    return total_reward.

================================================================================
ğŸš€ IMPLEMENTACE - UNIVERSAL REWARD SYSTEM (JAX Pure)
================================================================================

Krok 1: RozÅ¡Ã­Å™enÃ­ EnvState (PamÄ›Å¥ pro PBRS/Smoothness) ğŸ§ 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MusÃ­me si pamatovat, co bylo v minulÃ©m kroku.

Soubor: entropy/core/pure_structures.py

```python
@struct.dataclass
class EnvState:
    pos: jnp.ndarray          # [N, 2]
    rot: jnp.ndarray          # [N, 1]
    vel: jnp.ndarray          # [N, 2]
    ang_vel: jnp.ndarray      # [N, 1]
    
    # NOVÃ‰ POLOÅ½KY PRO REWARD CALC:
    prev_pos: jnp.ndarray     # [N, 2]  <- Pro PBRS (zlepÅ¡enÃ­ pozice)
    prev_action: jnp.ndarray  # [N, 2]  <- Pro Action Smoothness
    
    # Pro Push task (volitelnÃ©):
    box_pos: jnp.ndarray      # [1, 2]
    prev_box_pos: jnp.ndarray # [1, 2]
    
    step_count: int
    done: bool
```

DÅ¯leÅ¾itÃ©: PÅ™i reset() nastav prev_pos = pos a prev_action = zeros.

Krok 2: PÅ™edÃ¡vÃ¡nÃ­ progress (Curriculum) â³
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
V trÃ©ninkovÃ©m loopu:

```python
for epoch in range(total_epochs):
    progress = epoch / total_epochs  # 0.0 -> 1.0
    
    # PÅ™edej do rollout funkce
    rollout_fn = partial(rollout, progress=progress)
    # ...
```

Krok 3: calculate_universal_reward ğŸ§®
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Soubor: entropy/core/reward_system.py (NOVÃ)

```python
import jax
import jax.numpy as jnp

def calculate_universal_reward(state, action, params, config, progress):
    """
    VypoÄÃ­tÃ¡ reward pro vÅ¡echny agenty najednou.
    
    Args:
        state: EnvState (s novou pozicÃ­ a uloÅ¾enou prev_pos)
        action: AktuÃ¡lnÄ› aplikovanÃ¡ akce [N, action_dim]
        params: EnvParams (agent_radius, target_radius, gamma...)
        config: RewardConfig (w_dist, gamma...)
        progress: 0.0-1.0 (pro curriculum decay)
    """
    
    # === A. PHYSICS REGULARIZATION (VÅ¾dy aktivnÃ­) ===
    
    # 1. Action Smoothness (TlumÃ­ Å¡kubÃ¡nÃ­)
    action_diff = jnp.sum(jnp.square(action - state.prev_action), axis=-1)
    smoothness_penalty = action_diff * 0.1

    # 2. Energy Cost
    energy_penalty = jnp.sum(jnp.square(action), axis=-1) * 0.01

    # 3. Existence Penalty (Motivace k rychlosti)
    time_penalty = 0.01

    # === B. COLLISION DETECTION (On-the-fly) ===
    N = state.pos.shape[0]
    dist_matrix = jnp.linalg.norm(state.pos[:, None] - state.pos[None, :], axis=-1)
    dist_matrix = dist_matrix + jnp.eye(N) * 1e6  # Ignoruj sebe
    min_dist = jnp.min(dist_matrix, axis=-1)
    collision_mask = min_dist < (params.agent_radius * 2.0)
    collision_penalty = collision_mask * 10.0

    # === C. TASK REWARDS (Switch) ===

    # ÃšKOL 0: NAVIGACE
    def reward_nav():
        curr_dist = jnp.linalg.norm(state.pos - state.target, axis=-1)
        prev_dist = jnp.linalg.norm(state.prev_pos - state.target, axis=-1)
        
        # PBRS Shaping
        shaping = (config.gamma * (-curr_dist) - (-prev_dist)) * 0.5
        
        # Sparse Goal
        at_goal = curr_dist < params.target_radius
        sparse = at_goal * 200.0
        
        return shaping + sparse

    # ÃšKOL 1: SEARCH (Exploration)
    def reward_search():
        is_visible = state.target_visible 
        found = is_visible * 100.0
        movement = jnp.linalg.norm(state.vel, axis=-1) * 0.05
        return found + movement

    # ÃšKOL 2: PUSH (Manipulace)
    def reward_push():
        box_curr = jnp.linalg.norm(state.box_pos - state.target, axis=-1)
        box_prev = jnp.linalg.norm(state.prev_box_pos - state.target, axis=-1)
        box_shaping = (config.gamma * (-box_curr) - (-box_prev)) * 2.0
        agent_to_box = jnp.linalg.norm(state.pos - state.box_pos, axis=-1)
        agent_shaping = -agent_to_box * 0.05
        return box_shaping + agent_shaping

    # VÃ½bÄ›r rewardu podle Task ID
    task_reward = jax.lax.switch(
        params.task_id,
        [reward_nav, reward_search, reward_push]
    )

    # === D. CURRICULUM (PostupnÃ© odbourÃ¡vÃ¡nÃ­) ===
    alpha = 1.0 - (0.8 * progress)
    aux_reward = alpha * (-collision_penalty)

    # === FINAL SUM ===
    total_reward = (
        task_reward + 
        aux_reward - 
        smoothness_penalty - 
        energy_penalty - 
        time_penalty
    )
    
    return total_reward
```

Krok 4: Integrace do step() ğŸ”§
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Soubor: entropy/core/pure_engine.py

```python
def step(state: EnvState, action: jnp.ndarray, params: EnvParams, progress: float):
    # 1. ZÃ¡loha pro PBRS
    prev_pos = state.pos
    prev_box_pos = state.box_pos  # Pokud mÃ¡Å¡ Push task
    
    # 2. Fyzika
    next_state = physics_step(state, action, params)
    
    # 3. UloÅ¾enÃ­ historie do novÃ©ho stavu
    next_state = next_state.replace(
        prev_pos=prev_pos,
        prev_action=action,
        prev_box_pos=prev_box_pos
    )
    
    # 4. VÃ½poÄet Rewardu
    reward = calculate_universal_reward(next_state, action, params, config, progress)
    
    # 5. Done
    done = next_state.step_count >= params.max_steps
    
    return next_state, reward, done
```

================================================================================
ğŸ“Š SHRNUTÃ REWARD KOMPONENT
================================================================================

| Komponenta         | VÃ¡ha   | Popis                              |
|--------------------|--------|------------------------------------|
| Sparse Goal        | 200.0  | HlavnÃ­ cÃ­l, vÅ¾dy platÃ­             |
| PBRS Shaping       | 0.5    | NavÃ¡dÄ›nÃ­ (klesÃ¡ s curriculum)      |
| Smoothness Penalty | -0.1   | Penalizace za Å¡kubÃ¡nÃ­              |
| Energy Penalty     | -0.01  | Penalizace za plÃ½tvÃ¡nÃ­ energiÃ­     |
| Collision Penalty  | -10.0  | TvrdÃ¡ penalizace za kolize         |
| Time Penalty       | -0.01  | Motivace k rychlosti               |

Curriculum Alpha: 1.0 â†’ 0.2 (auxiliary rewards klesajÃ­, sparse zÅ¯stÃ¡vÃ¡)

================================================================================
ğŸ§¹ FINÃLNÃ JAX ARCHITEKTURA (Clean Structure)
================================================================================

1. EnvParams (VÅ¡e pro JIT) ğŸ“œ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VÅ¡echny konstanty ovlivÅˆujÃ­cÃ­ reward a fyziku. OznaÄenÃ© jako static_argnums.

```python
@struct.dataclass
class EnvParams:
    # Fyzika
    dt: float = 0.1
    agent_radius: float = 1.0
    arena_size: float = 800.0
    
    # Reward Hyperparametry
    gamma: float = 0.99
    w_dist: float = 1.0
    w_energy: float = 0.01
    w_smooth: float = 0.1
    w_collision: float = 10.0
    
    # Ãškol
    task_id: int = 0            # 0=Nav, 1=Search, 2=Push
    target_radius: float = 5.0
```

2. EnvState (UniverzÃ¡lnÃ­ pro vÅ¡echny Ãºkoly) ğŸ“¦
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FixnÃ­ struktura s [N, 2] targety pro flexibilitu.

```python
@struct.dataclass
class EnvState:
    # Kinematika
    pos: jnp.ndarray          # [N, 2]
    vel: jnp.ndarray          # [N, 2]
    rot: jnp.ndarray          # [N, 1]
    
    # Historie (PBRS + Smoothness)
    prev_pos: jnp.ndarray     # [N, 2]
    prev_action: jnp.ndarray  # [N, 2]
    
    # Ãškoly
    target: jnp.ndarray       # [N, 2] â€” VÅ½DY dimenze N!
    box_pos: jnp.ndarray      # [1, 2]
    prev_box_pos: jnp.ndarray # [1, 2]
    
    # Senzory
    lidar: jnp.ndarray        # [N, 32]
    target_visible: jnp.ndarray # [N, 1]
    
    # Meta
    step_count: int
    done: bool
```

DÅ®LEÅ½ITÃ‰: target je [N, 2] i pro shared goal (Flocking):
```python
# Shared goal (Flocking)
single_target = jnp.array([100.0, 100.0])
state_target = jnp.tile(single_target, (num_agents, 1))  # [N, 2]

# Per-agent goals (Routing)
state_target = random_targets  # [N, 2]
```
jnp.tile je zero-cost (broadcast, Å¾Ã¡dnÃ© kopie v pamÄ›ti).

3. Progress Propagace (Training Loop) ğŸ”„
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
progress jako argument do JIT funkce.

```python
@jax.jit
def update_step(state, params, opt_state, key, progress):
    # Rollout s progressem
    def policy_step(state, key):
        next_state, reward, done = env.step(state, action, params, progress)
        return next_state, ...
    # ...
    return new_state, info

# Python loop
for epoch in range(total_epochs):
    progress = epoch / total_epochs
    state, metrics = update_step(state, params, opt_state, key, progress)
```

================================================================================
âœ… CHECKLIST PÅ˜ED IMPLEMENTACÃ
================================================================================

| PoloÅ¾ka                              | Status |
|--------------------------------------|--------|
| EnvParams s reward hyperparametry    | âœ“      |
| EnvState s [N, 2] target             | âœ“      |
| prev_pos, prev_action v state        | âœ“      |
| progress jako argument               | âœ“      |
| Task switch (jax.lax.switch)         | âœ“      |
| PBRS shaping                         | âœ“      |
| Collision on-the-fly                 | âœ“      |
| Curriculum alpha decay               | âœ“      |

================================================================================
âœ… STATUS: READY FOR IMPLEMENTATION
================================================================================