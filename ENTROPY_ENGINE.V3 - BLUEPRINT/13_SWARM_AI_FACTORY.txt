================================================================================
                    13_SWARM_AI_FACTORY.txt
                    "V√Ωrobn√≠ Linka na Inteligenci" (Assembly Line)
================================================================================

================================================================================
1. FILOSOFIE: AUTOMATIZOVAN√Å TOV√ÅRNA
================================================================================

V3 AI Factory nen√≠ jen skript pro tr√©nink, je to ORCHESTR√ÅTOR.
M√≠sto tr√©nov√°n√≠ v≈°eho najednou (End-to-End RL) rozdƒõlujeme uƒçen√≠ na logick√© bloky
(stanice), kter√© na sebe navazuj√≠.

KL√çƒåOV√â V√ùHODY:
- Rychlost: Vy≈ôazen√≠ ≈°patn√Ωch model≈Ø po 5 minut√°ch (Stanice 1), ne po 10 hodin√°ch.
- Stabilita: Ka≈æd√° stanice m√° "Frozen Experts" - nov√© dovednosti neniƒç√≠ ty star√©.
- QA Gates: Ka≈æd√° stanice m√° p≈ô√≠sn√° krit√©ria (Pass/Fail) pro postup.
- Rodokmen: Ka≈æd√Ω model m√° dohledatelnou historii (Lineage).

================================================================================
2. STATIONS (Zast√°vky Linky)
================================================================================

üìç STATION 0: THE ORACLE FACTORY (Offline Data Generation) üîÆ
-------------------------------------------------------------
C√çL: Vygenerovat masivn√≠ dataset perfektn√≠ch trajektori√≠ BEZ zapojen√≠ AI.
VSTUP: N√°hodnƒõ generovan√© mapy/sc√©n√°≈ôe.
METODA: Oracle-Guided Learning / Expert Distillation.
V√ùSTUP: 100,000+ demonstraƒçn√≠ch z√°znam≈Ø (State, Action) p√°r≈Ø.
ƒåAS: Bƒõ≈æ√≠ 24/7 na pozad√≠, nez√°visle na tr√©ninku.

ARCHITEKTURA ORACLE:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     THE ORACLE (V≈°evƒõdouc√≠ Expert)                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  PRIVILEGED INFO:                                                 ‚îÇ
‚îÇ    - Vid√≠ skrz zdi (pln√° mapa)                                    ‚îÇ
‚îÇ    - Zn√° p≈ôesnou polohu c√≠le                                      ‚îÇ
‚îÇ    - Zn√° budouc√≠ pohyb nep≈ô√°tel (predikce)                        ‚îÇ
‚îÇ    - Glob√°ln√≠ znalost v≈°ech agent≈Ø                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ALGORITMY:                                                       ‚îÇ
‚îÇ    - A* (pathfinding pro navigaci)                                ‚îÇ
‚îÇ    - RRT* (pl√°nov√°n√≠ pohybu v dynamick√©m prost≈ôed√≠)               ‚îÇ
‚îÇ    - Formation Planner (geometrick√© formace)                      ‚îÇ
‚îÇ    - Cooperative Scheduler (kdo kam jde)                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  V√ùSTUP:                                                          ‚îÇ
‚îÇ    - Perfektn√≠ akce (motor commands)                              ‚îÇ
‚îÇ    - Perfektn√≠ komunikace (ground truth tokeny)                   ‚îÇ
‚îÇ    - Metadata (proƒç tato akce)                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

PROƒå ORACLE A NE ƒåLOVƒöK (Teleoperation)?
  - ≈†k√°lovatelnost: Oracle generuje data 24/7, ƒçlovƒõk vydr≈æ√≠ 10 minut.
  - P≈ôesnost: Oracle neudƒõl√° p≈ôeklik ani jitter.
  - Konzistence: 100,000 perfektn√≠ch pr≈Øchod≈Ø bez √∫navy.
  - Reprodukovatelnost: Deterministick√©, lze opakovat.

IMPLEMENTACE:
```python
class OracleFactory:
    """
    Offline gener√°tor demonstraƒçn√≠ch dat.
    Bƒõ≈æ√≠ P≈òED spu≈°tƒõn√≠m AI tr√©ninku.
    """
    def __init__(self, config):
        self.scenario_generator = ScenarioGenerator(config)
        self.oracle = PrivilegedOracle(config)
        self.demo_buffer = DemonstrationBuffer(capacity=500_000)
        
    def generate_demos(self, num_episodes=100_000, maps_per_episode=1):
        """Generuj N epizod demonstraƒçn√≠ch dat."""
        for episode in tqdm(range(num_episodes)):
            # 1. Vygeneruj n√°hodn√Ω sc√©n√°≈ô
            scenario = self.scenario_generator.generate(
                map_type=random.choice(["maze", "open", "rooms"]),
                num_agents=random.randint(2, 10),
                difficulty=random.uniform(0.3, 1.0)
            )
            
            # 2. Spus≈• Oracle (BEZ AI)
            state = self.env.reset(scenario)
            trajectory = []
            
            while not state.done:
                # Oracle vid√≠ PLN√ù STATE (privileged)
                oracle_action = self.oracle.act(state.full_state)
                
                # Ulo≈æ p√°r (observation, action)
                # KL√çƒå: Observation je to, co by vidƒõla AI (partial)
                trajectory.append(OracleDemo(
                    observation=state.get_agent_observation(),  # Lidar, etc.
                    action=oracle_action,
                    metadata={
                        "goal_visible": state.is_goal_visible(),
                        "path_length": self.oracle.current_path_length,
                        "scenario_difficulty": scenario.difficulty
                    }
                ))
                
                state = self.env.step(oracle_action)
            
            # 3. Ulo≈æ pouze √∫spƒõ≈°n√© trajektorie
            if state.goal_reached:
                self.demo_buffer.add_trajectory(trajectory)
                
        # 4. Export pro Factory
        self.demo_buffer.save("oracle_demos.pkl")
        return self.demo_buffer.stats()


class PrivilegedOracle(ExpertPolicy):
    """
    V≈°evƒõdouc√≠ expert pro generov√°n√≠ demonstrac√≠.
    Kombinuje v√≠ce specializovan√Ωch pl√°novaƒç≈Ø.
    """
    def __init__(self, config):
        self.navigator = AStarNavigator(resolution=config.astar_resolution)
        self.formation = FormationPlanner()
        self.communicator = RuleBasedCommunication()
        
    def act(self, full_state: WorldState) -> OracleAction:
        """Vr√°t√≠ perfektn√≠ akci na z√°kladƒõ pln√©ho stavu."""
        
        # Motor commands (kam jet)
        motor_action = self.navigator.compute_path(
            start=full_state.agent_positions,
            goals=full_state.goal_positions,
            obstacles=full_state.all_obstacles  # Vid√≠ i za rohem!
        )
        
        # Communication (co ≈ô√≠ct)
        comm_action = self.communicator.get_ground_truth_message(
            full_state, 
            agent_roles=full_state.roles
        )
        
        return OracleAction(motor=motor_action, comm=comm_action)
```

INTEGRACE DO FACTORY:
  - Oracle demos se naƒçtou jako "warm-start" pro Station 1 a 2.
  - Behavioral Cloning (Supervised) p≈ôed RL drasticky zrychl√≠ konvergenci.
  - AI zaƒç√≠n√° s "intuic√≠" Oracla, ne jako Tabula Rasa.

```python
# V FactoryManager - p≈ôed spu≈°tƒõn√≠m Station 1:
def load_oracle_warmstart(self, path="oracle_demos.pkl"):
    """Naƒçti pre-generovan√© Oracle demonstrace."""
    demos = DemonstrationBuffer.load(path)
    print(f"üì¶ Loaded {len(demos)} Oracle demonstrations")
    
    # Pre-train pomoc√≠ Behavioral Cloning
    self.pretrain_bc(demos, epochs=5)
    print("üéì Behavioral Cloning warm-start complete")
```

HARD STATES GENERATION (C√≠len√© pokryt√≠ obt√≠≈æn√Ωch situac√≠):
  Klasick√Ω Oracle generuje n√°hodnƒõ ‚Üí m≈Ø≈æe chybƒõt kritick√© edge cases.
  ≈òe≈°en√≠: Aktivnƒõ generuj demos pro "hard states":
  
  ```python
  class HardStatesGenerator:
      """C√≠lenƒõ generuje obt√≠≈æn√© situace pro lep≈°√≠ coverage."""
      
      HARD_SCENARIO_TYPES = [
          "narrow_passage",     # √özk√Ω pr≈Øchod (1 agent ≈°√≠≈ôka)
          "dead_end_escape",    # Slep√° uliƒçka, nutn√Ω backtrack
          "multi_agent_cross",  # K≈ô√≠≈æen√≠ trajektori√≠ v√≠ce agent≈Ø
          "dynamic_obstacle",   # Pohybuj√≠c√≠ se p≈ôek√°≈æka
          "goal_behind_wall",   # C√≠l viditeln√Ω, ale nep≈ô√≠m√° cesta
          "formation_squeeze",  # Formace mus√≠ projet √∫zk√Ωm m√≠stem
      ]
      
      def generate_hard_states(self, num_per_type=5000):
          """Generuj c√≠lenƒõ tƒõ≈æk√© situace."""
          for scenario_type in self.HARD_SCENARIO_TYPES:
              for _ in range(num_per_type):
                  scenario = self._create_scenario(scenario_type)
                  yield scenario
                  
      def harvest_failure_states(self, student_rollouts):
          """
          Sb√≠rej stavy kde student selhal ‚Üí regeneruj Oracle demos.
          Tohle ≈ôe≈°√≠ covariate shift!
          """
          failure_states = [
              rollout.state for rollout in student_rollouts 
              if rollout.collision or not rollout.goal_reached
          ]
          return self._relabel_with_oracle(failure_states)
  ```

COVERAGE-AWARE BC (Stabilnƒõj≈°√≠ RL Finetuning):
  Klasick√© BC tr√©nuje na jedinou "spr√°vnou" akci ‚Üí √∫zk√° distribuce.
  RL pak ≈°patnƒõ fine-tunuje, proto≈æe policy nem√° z ƒçeho "h√Ωbat".
  
  ≈òe≈°en√≠: Stochastick√° policy s entropy regularizac√≠:
  
  ```python
  def coverage_aware_bc_loss(policy, demos, entropy_coef=0.01):
      """
      BC loss s entropy regularizac√≠ pro ≈°ir≈°√≠ coverage.
      """
      # Standardn√≠ BC loss
      pred_actions = policy(demos.observations)
      bc_loss = mse_loss(pred_actions, demos.expert_actions)
      
      # Entropy bonus - udr≈æuj policy ≈°irokou
      entropy = -jnp.sum(policy.log_prob(pred_actions))
      
      # Label smoothing - nep≈ôefituj na jedin√© ≈ôe≈°en√≠
      smoothed_targets = (
          0.9 * demos.expert_actions + 
          0.1 * jax.random.normal(key, demos.expert_actions.shape) * 0.1
      )
      smooth_loss = mse_loss(pred_actions, smoothed_targets)
      
      return bc_loss + 0.5 * smooth_loss - entropy_coef * entropy
  ```

KONFIGURACE:
```yaml
# configs/factory/oracle.yaml

oracle_factory:
  enabled: true
  
  generation:
    num_episodes: 100_000
    maps_per_difficulty:
      easy: 30_000
      medium: 50_000
      hard: 20_000
    
  # === NOV√â: Hard States ===
  hard_states:
    enabled: true
    scenarios_per_type: 5000
    types: ["narrow_passage", "dead_end", "multi_cross", "dynamic_obs"]
    harvest_failures: true  # Sb√≠rej failure states ze student≈Ø
    
  oracle:
    navigator:
      algorithm: "astar"
      resolution: 10
      allow_diagonal: true
    formation:
      enabled: true
      types: ["line", "v", "circle"]
    communication:
      inject_ground_truth: true
      
  # === NOV√â: Streamovateln√Ω form√°t ===
  storage:
    path: "data/oracle_demos/"
    format: "tfrecord"     # Streamovateln√©, ne pickle!
    shard_size: 10_000     # Demos per shard
    compression: "gzip"    # Men≈°√≠ velikost
    memory_map: true       # Rychl√© naƒç√≠t√°n√≠
    
  # === NOV√â: Coverage-aware BC ===
  warmstart:
    enabled: true
    bc_epochs: 5
    bc_learning_rate: 1e-3
    entropy_coef: 0.01     # Entropy regularizace
    label_smoothing: 0.1   # ≈†ir≈°√≠ distribuce
```

V√ùHODY ORACLE P≈ò√çSTUPU:
  ‚úÖ Zero-Shot Performance: AI nen√≠ tup√° na zaƒç√°tku RL.
  ‚úÖ ≈òe≈°en√≠ explorace: Oracle uk√°≈æe jak se dostat do vz√°cn√Ωch stav≈Ø.
  ‚úÖ Konzistentn√≠ data: ≈Ω√°dn√Ω lidsk√Ω jitter/chyby.
  ‚úÖ Parallel generation: M≈Ø≈æe bƒõ≈æet na CPU zat√≠mco GPU tr√©nuje.
  ‚úÖ Hard States: C√≠len√© pokryt√≠ edge cases.
  ‚úÖ Coverage-aware: ≈†ir≈°√≠ policy pro lep≈°√≠ RL finetuning.

OMEZEN√ç:
  ‚ö†Ô∏è Oracle ne≈ôe≈°√≠ real-time reakce (to p≈ôijde v RL f√°zi).
  ‚ö†Ô∏è Oracle nev√≠ o ostatn√≠ch AI agentech (kooperace a≈æ Station 4).
  ‚ö†Ô∏è Oracle je pomal√Ω (1 FPS), AI mus√≠ b√Ωt rychl√° (60 FPS).

-------------------------------------------------------------------------------

üìç STATION 1: THE KINDERGARTEN (Z√°kladn√≠ slovn√≠k)
-----------------------------------------------
C√çL: Nauƒçit agenta v√Ωznam slov (Grounding) bez nutnosti pohybu.
VSTUP: Tabula Rasa model.
METODA: Supervised Learning (Hand of God Injection).
√öKOL: "Co vid√≠≈°?" (Agent stoj√≠, dost√°v√° n√°hodn√© sensory -> mus√≠ vybrat spr√°vn√Ω token).
QA GATE: 99% p≈ôesnost v pojmenov√°n√≠ zn√°m√Ωch objekt≈Ø.
ƒåAS: ~5-10 min (RTX 3060).

PRIORITIZED SAMPLING (Rychlej≈°√≠ konvergence):
  M√≠sto uniformn√≠ho samplov√°n√≠ demos p≈ôeva≈æuj vzorky s vysokou chybou:
  
  ```python
  class PrioritizedDemoSampler:
      def sample(self, demos, model, batch_size=256):
          # Spoƒç√≠tej chyby pro v≈°echny demos
          predictions = model(demos.observations)
          errors = jnp.abs(predictions - demos.targets)
          
          # Priority = chyba ^ alpha (alpha ~ 0.6)
          priorities = errors ** 0.6
          probs = priorities / priorities.sum()
          
          # Sample podle priorit
          indices = jax.random.choice(key, len(demos), p=probs, shape=(batch_size,))
          return demos[indices]
  ```
  
  V√Ωhoda: 2-3x rychlej≈°√≠ konvergence bez zmƒõny modelu.

-------------------------------------------------------------------------------

üìç STATION 2: THE GYM (Hrub√° motorika)
--------------------------------------
C√çL: Perfektn√≠ pohyb a navigace k c√≠li bez n√°raz≈Ø.
VSTUP: Model ze Stanice 1 (nebo Tabula Rasa).
METODA: Iterativn√≠ DAgger (ne jednor√°zov√© BC!).
√öKOL: "Dosta≈à se k c√≠li v bludi≈°ti." (Komunikace je vypnut√°).
QA GATE: 95% √∫spƒõ≈°nost na OUT-OF-DISTRIBUTION map√°ch.
ƒåAS: ~15-30 min.

ITERATIVN√ç DAgger LOOP (≈òe≈°en√≠ Compounding Errors):
  Probl√©m s klasick√Ωm BC: Student udƒõl√° malou chybu ‚Üí ocitne se v nov√©m stavu
  ‚Üí Expert nikdy neukazoval co dƒõlat v tomto stavu ‚Üí vƒõt≈°√≠ chyba ‚Üí spir√°la smrti.
  
  DAgger (Dataset Aggregation) tohle ≈ôe≈°√≠ iterativn√≠m sbƒõrem dat:
  
  ```
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ                    DAGGER ITERATIVE LOOP                       ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ  Round 0: BC warmstart z Oracle demos (Station 0)             ‚îÇ
  ‚îÇ           ‚Üì                                                    ‚îÇ
  ‚îÇ  Round k: 1. Rollout(student) na nov√Ωch map√°ch                ‚îÇ
  ‚îÇ           2. Expert labelu CO BY UDƒöLAL v tƒõchto stavech      ‚îÇ
  ‚îÇ           3. Append (student_states, expert_actions) do D     ‚îÇ
  ‚îÇ           4. Retrain na D ‚à™ D_original                         ‚îÇ
  ‚îÇ           5. Opakuj dokud OOD_success_rate nez≈Østane stabiln√≠ ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ```
  
  ```python
  class DAggerTrainer:
      def train(self, model, oracle, initial_demos, max_rounds=5):
          dataset = initial_demos.copy()
          
          for round_k in range(max_rounds):
              print(f"üîÑ DAgger Round {round_k}")
              
              # 1. Student rollout (sb√≠rej stavy kter√© nav≈°t√≠v√≠)
              student_states = []
              for episode in range(100):
                  state = env.reset(random_map())
                  while not state.done:
                      action = model.act(state.observation)  # Student ≈ô√≠d√≠!
                      student_states.append(state.observation)
                      state = env.step(action)
              
              # 2. Expert labeluje tyto stavy
              expert_labels = [oracle.act(s) for s in student_states]
              
              # 3. Append do datasetu
              dataset.add(student_states, expert_labels)
              
              # 4. Retrain
              model.fit(dataset, epochs=3)
              
              # 5. Evaluace na OOD map√°ch
              ood_success = evaluate_ood(model)
              if ood_success > 0.95:
                  print(f"‚úÖ DAgger converged at round {round_k}")
                  break
                  
          return model
  ```
  
  KL√çƒå: Expert labeluje stavy KDE BYL STUDENT, ne stavy z Oracle trajektori√≠.
  Tohle eliminuje covariate shift / compounding errors.

üìç STATION 3: THE LANGUAGE SCHOOL (Multimod√°ln√≠ f√∫ze)
-----------------------------------------------------
C√çL: Spojen√≠ akce a slova (Konzistence).
VSTUP: Frozen Encoders ze S1 (Comm) a S2 (Motor).
METODA: Trainable Fusion Layer.
√öKOL: "≈ò√≠kej co dƒõl√°≈°." (Pokud jde k j√≠dlu, MUS√ç vys√≠lat token FOOD).
QA GATE: Konzistence mezi vys√≠lan√Ωm tokenem a smƒõrem pohybu > 90%.
ƒåAS: ~30 min.

üìç STATION 4: THE TEAM BUILDING (Kooperace)
-------------------------------------------
C√çL: Emergentn√≠ kooperace a swarm dynamika.
VSTUP: Model ze Stanice 3.
METODA: Multi-Agent PPO (MAPPO) s asistenc√≠ (Hand of God).
√öKOL: "Tlaƒçte tƒõ≈æk√Ω objekt spoleƒçnƒõ", "Uvolni cestu", "Bra≈à c√≠l".
QA GATE: Splnƒõn√≠ kooperativn√≠ho sc√©n√°≈ôe (Reward Threshold).
ƒåAS: ~1-3 hodiny.

MAPPO OPTIMALIZACE (Actor/Critic Split):
  Bottleneck v MAPPO je ƒçasto critic (m√° glob√°ln√≠ vstup v≈°ech agent≈Ø).
  ≈òe≈°en√≠: Asymetrick√Ω update budget:
  
  ```python
  class OptimizedMAPPO:
      def __init__(self, config):
          self.actor_updates_per_step = 4   # Actor ƒçastƒõji
          self.critic_updates_per_step = 1  # Critic m√©nƒõ ƒçasto
          
      def update(self, rollout_buffer):
          # Critic update (1x za step, men≈°√≠ s√≠≈•)
          critic_loss = self.update_critic(rollout_buffer)
          
          # Actor update (4x za step, rychlej≈°√≠)
          for _ in range(self.actor_updates_per_step):
              actor_loss = self.update_actor(rollout_buffer)
              
          return actor_loss, critic_loss
  ```
  
  V√Ωhoda: Lep≈°√≠ FPS tr√©ninku + stabilnƒõj≈°√≠ credit assignment.
  
  KONFIGURACE:
  ```yaml
  mappo:
    actor:
      lr: 3e-4
      updates_per_step: 4
      hidden_dims: [256, 256]
    critic:
      lr: 1e-3
      updates_per_step: 1
      hidden_dims: [512, 256]  # Vƒõt≈°√≠, ale m√©nƒõ updat≈Ø
      global_state: true       # Vid√≠ v≈°echny agenty
  ```

üìç STATION 5: THE WAR ROOM (Stress Test / QA)
---------------------------------------------
C√çL: Robustnost a certifikace Master Modelu.
VSTUP: Model ze Stanice 4.
METODA: "Chaos Monkey" (Adversarial Training).
√öKOL: Senzorick√© v√Ωpadky, zmƒõna fyzik√°ln√≠ch konstant, nep≈ô√°tel≈°t√≠ agenti.
QA GATE: P≈ôe≈æit√≠ v 95% stresov√Ωch sc√©n√°≈ô≈Ø.
ƒåAS: ~30-60 min.

-------------------------------------------------------------------------------

üìç STATION 5.5: DOMAIN RANDOMIZATION CERTIFICATION üé≤
------------------------------------------------------
C√çL: Sim-to-Real robustnost p≈ôes systematickou randomizaci.
VSTUP: Model ze Stanice 5.
METODA: Domain Randomization (DR) + Generalization Gap anal√Ωza.
√öKOL: Udr≈æet v√Ωkon nap≈ô√≠ƒç rodinou MDP s r≈Øzn√Ωmi parametry.
QA GATE: Generalization Gap < 10%.
V√ùSTUP: "Certified Master Model v1.0" (Real-World Ready).

CO JE DOMAIN RANDOMIZATION?
  M√≠sto ladƒõn√≠ simul√°toru na p≈ôesn√© fyzik√°ln√≠ konstanty (tƒõ≈æk√©, k≈ôehk√©)
  tr√©nujeme na ROZSAHU parametr≈Ø. Model pak generalizuje na "re√°ln√©" hodnoty.
  
  P≈ô√≠klad: M√≠sto friction=0.5 tr√©nujeme na friction ‚àà [0.3, 0.8].
  Model pak zvl√°dne i friction=0.55 v re√°ln√©m svƒõtƒõ.

RANDOMIZOVAN√â PARAMETRY:
```python
@dataclass
class DomainRandomizationConfig:
    """Rozsahy pro randomizaci sim-to-real."""
    
    # === Fyzika ===
    friction_range: Tuple[float, float] = (0.3, 1.5)
    motor_power_variance: float = 0.20      # ¬±20%
    wheel_slip_range: Tuple[float, float] = (0.0, 0.15)
    agent_mass_variance: float = 0.15       # ¬±15%
    
    # === Senzory ===
    lidar_dropout_prob: float = 0.10        # 10% chybƒõj√≠c√≠ch paprsk≈Ø
    lidar_noise_stddev: float = 0.05        # Gaussovsk√Ω ≈°um
    camera_blur_prob: float = 0.05          # Motion blur
    sensor_delay_ms: Tuple[int, int] = (0, 30)  # Zpo≈ædƒõn√≠
    
    # === Komunikace ===
    comm_delay_ms: Tuple[int, int] = (0, 50)    # Latence zpr√°v
    comm_dropout_prob: float = 0.05             # Ztracen√© zpr√°vy
    comm_noise_stddev: float = 0.02             # ≈†um v payloadu
    
    # === Prost≈ôed√≠ ===
    lighting_variance: float = 0.3          # Pro vision modely
    texture_randomization: bool = True      # N√°hodn√© textury
```

IMPLEMENTACE:
```python
class DomainRandomizer:
    def __init__(self, config: DomainRandomizationConfig):
        self.config = config
        
    def randomize_physics(self, env, rng):
        """N√°hodnƒõ nastav√≠ fyzik√°ln√≠ parametry."""
        env.friction = jax.random.uniform(
            rng, minval=self.config.friction_range[0],
            maxval=self.config.friction_range[1]
        )
        env.motor_power *= 1.0 + jax.random.uniform(
            rng, minval=-self.config.motor_power_variance,
            maxval=self.config.motor_power_variance
        )
        
    def randomize_sensors(self, observation, rng):
        """P≈ôidej ≈°um a dropout do senzor≈Ø."""
        # Lidar dropout
        dropout_mask = jax.random.bernoulli(
            rng, 1 - self.config.lidar_dropout_prob,
            shape=observation.lidar.shape
        )
        observation.lidar *= dropout_mask
        
        # Gaussovsk√Ω ≈°um
        noise = jax.random.normal(rng, observation.lidar.shape)
        observation.lidar += noise * self.config.lidar_noise_stddev
        
        return observation
```

GENERALIZATION GAP METRIKA:
  Mƒõ≈ô√≠me rozd√≠l v√Ωkonu mezi "ƒçistou" a "randomizovanou" simulac√≠:
  
  ```python
  def compute_generalization_gap(model):
      # V√Ωkon na ƒçist√© simulaci (default params)
      clean_success = evaluate(model, randomize=False)
      
      # V√Ωkon na randomizovan√© simulaci
      dr_success = evaluate(model, randomize=True)
      
      # Gap by mƒõl b√Ωt < 10%
      gap = abs(clean_success - dr_success)
      return gap
  ```
  
  QA GATE: `generalization_gap < 0.10`
  
  Pokud gap > 10%, model je p≈ô√≠li≈° "overfit" na ƒçistou simulaci
  a nep≈ôe≈æije v re√°ln√©m svƒõtƒõ.

KONFIGURACE:
```yaml
# configs/factory/domain_randomization.yaml

domain_randomization:
  enabled: true
  
  physics:
    friction: [0.3, 1.5]
    motor_power_variance: 0.20
    wheel_slip: [0.0, 0.15]
    mass_variance: 0.15
    
  sensors:
    lidar_dropout: 0.10
    lidar_noise: 0.05
    delay_ms: [0, 30]
    
  communication:
    delay_ms: [0, 50]
    dropout: 0.05
    
  qa_gate:
    max_generalization_gap: 0.10
    min_dr_success_rate: 0.85
```

V√ùSTUP: "Certified Master Model v1.0" (Real-World Ready).

================================================================================
2.6 ADAPTIVE SKEPTICISM DIAL (Dynamick√Ω Skepticismus) üéöÔ∏è
================================================================================

Nap≈ô√≠ƒç linkou graduelnƒõ roste "error_rate" experta (viz 11_HAND_OF_GOD.txt).
To vytv√°≈ô√≠ "Ovladaƒç Skepse" - AI se uƒç√≠ nez√°visl√©mu rozhodov√°n√≠.

PROBL√âM S FIXN√çM error_rate:
  Fixn√≠ hodnoty (2%, 5%, 10%) ignoruj√≠ aktu√°ln√≠ stav uƒçen√≠.
  ‚Üí P≈ô√≠li≈° brzy = rozbit√Ω model (AI se nezvl√°d√° uƒçit z√°klady)
  ‚Üí P≈ô√≠li≈° pozdƒõ = pomal√© uƒçen√≠ skepticismu

≈òE≈†EN√ç: ADAPTIVN√ç SKEPTICISM DIAL
  error_rate se dynamicky mƒõn√≠ podle performance metrik:
  
  ```python
  class AdaptiveSkepticismDial:
      """
      Dynamicky upravuje error_rate podle v√Ωkonu AI.
      """
      def __init__(self, config):
          self.error_rate = 0.0
          self.min_rate = 0.0
          self.max_rate = 0.20
          self.adjust_interval = 1000  # steps
          
          # Thresholds pro √∫pravu
          self.success_threshold = 0.95
          self.collision_threshold = 0.02
          self.crisis_threshold = 0.70
          
      def update(self, metrics):
          """
          Uprav error_rate podle aktu√°ln√≠ho v√Ωkonu.
          
          Args:
              metrics: {success_rate, collision_rate, goal_reached_rate}
          """
          # AI je stabiln√≠ a √∫spƒõ≈°n√° ‚Üí zvy≈° error_rate (v√≠ce skepticismu)
          if (metrics.success_rate > self.success_threshold and 
              metrics.collision_rate < self.collision_threshold):
              self.error_rate = min(
                  self.error_rate + 0.01, 
                  self.max_rate
              )
              print(f"üìà AI stable, increasing skepticism to {self.error_rate:.1%}")
              
          # AI se rozpad√° ‚Üí NOUZOV√ù BRAKEDOWN
          elif metrics.success_rate < self.crisis_threshold:
              self.error_rate = max(
                  self.error_rate - 0.02,
                  self.min_rate
              )
              print(f"üö® AI struggling, reducing skepticism to {self.error_rate:.1%}")
              
          return self.error_rate
          
      def get_error_rate(self):
          return self.error_rate
  ```

INTEGRACE DO FACTORY:
  ```python
  # V training loop:
  skepticism_dial = AdaptiveSkepticismDial(config)
  
  for step in range(max_steps):
      # Periodicky aktualizuj error_rate
      if step % skepticism_dial.adjust_interval == 0:
          metrics = compute_rolling_metrics(last_1000_episodes)
          current_error_rate = skepticism_dial.update(metrics)
          expert.set_error_rate(current_error_rate)
  ```

Z FIXN√çHO NA ADAPTIVN√ç (Doporuƒçen√° migrace):

üìç STANICE 1-2: Naivn√≠ D≈Øvƒõra
  error_rate = 0% (FIXN√ç, expert mus√≠ b√Ωt perfektn√≠)
  D≈Øvod: Na zaƒç√°tku AI nic neum√≠, pot≈ôebuje ƒçist√© vzory.

üìç STANICE 3: Zaƒç√≠n√° Pochybnost
  error_rate = ADAPTIVN√ç, start 0%, max 5%
  Zvy≈°uj jen kdy≈æ success_rate > 90%
  AI: Mus√≠ se nauƒçit verifikovat - vizu√°ln√≠ vstup (Lidar) > Command input.

üìç STANICE 4: Rostouc√≠ Skepse
  error_rate = ADAPTIVN√ç, start 2%, max 10%
  Emergency brake pod 70% success_rate
  AI: "Beru p≈ô√≠kaz jako n√°vrh, ne p≈ô√≠kaz."
  
üìç STANICE 5-5.5: Maxim√°ln√≠ Skepticismus
  error_rate = ADAPTIVN√ç, start 5%, max 20%
  Expert je "opil√Ω nebo nep≈ô√°telsk√Ω".
  Bonus: "Insubordination Reward" - pokud ignoruje ≈°patnou radu A spln√≠ c√≠l ‚Üí +5.0

KONFIGURACE:
```yaml
# configs/factory/skepticism.yaml

adaptive_skepticism:
  enabled: true
  
  per_station:
    station_1_2:
      mode: "fixed"
      error_rate: 0.0
    station_3:
      mode: "adaptive"
      start_rate: 0.0
      max_rate: 0.05
      success_threshold: 0.90
    station_4:
      mode: "adaptive"
      start_rate: 0.02
      max_rate: 0.10
      crisis_threshold: 0.70
    station_5:
      mode: "adaptive"
      start_rate: 0.05
      max_rate: 0.20
      
  adjust_interval: 1000
  increase_step: 0.01
  decrease_step: 0.02  # Rychlej≈°√≠ sn√≠≈æen√≠ p≈ôi krizi
```

V√ùSLEDEK:
  Robot, kter√Ω zastav√≠ na propasti m√≠sto slep√©ho n√°sledov√°n√≠ GPS.
  "GPS ≈ô√≠k√° doprava, kamery vid√≠ zeƒè ‚Üí IGNORUJ GPS."
  
  BONUS: Adaptivn√≠ syst√©m se s√°m reguluje - nezniƒç√≠ model p≈ô√≠li≈°
  agresivn√≠m l≈æiv√Ωm expertem v nevhodnou chv√≠li.

================================================================================
3. FACTORY MANAGER (Technick√° realizace)
================================================================================

ARCHITEKTURA: ODDƒöLEN√ç CORE A FACTORY
  Factory je nadstavba (Build System), kter√° pou≈æ√≠v√° Engine (Runtime) jako knihovnu.
  D≈Øvod: ƒåistota produkƒçn√≠ho k√≥du, oddƒõlen√≠ z√°vislost√≠.

  ```text
  ENTROPY_ENGINE.V3/
  ‚îú‚îÄ‚îÄ entropy/                  # CORE ENGINE (Runtime library) - "AUTO"
  ‚îÇ   ‚îú‚îÄ‚îÄ core/                 # Physics, ECS, World
  ‚îÇ   ‚îú‚îÄ‚îÄ env/                  # Runtime logika
  ‚îÇ   ‚îî‚îÄ‚îÄ brain/                # Inference-only logika (pro hru)
  ‚îÇ
  ‚îú‚îÄ‚îÄ factory/                  # SWARM FACTORY (Orchestrator) - "TOV√ÅRNA"
  ‚îÇ   ‚îú‚îÄ‚îÄ manager.py            # FactoryManager (≈ô√≠d√≠ linku)
  ‚îÇ   ‚îú‚îÄ‚îÄ stations/             # Logika jednotliv√Ωch stanic (Train Loops)
  ‚îÇ   ‚îú‚îÄ‚îÄ oracle/               # Oracle logika (A*, HardStates gener√°tor)
  ‚îÇ   ‚îî‚îÄ‚îÄ analysis/             # QA Gates, WandB reporting
  ‚îÇ
  ‚îî‚îÄ‚îÄ configs/
      ‚îú‚îÄ‚îÄ env/                  # Core configs
      ‚îî‚îÄ‚îÄ factory/              # Training pipelines
  ```

  Manager je orchestr√°tor, kter√Ω ≈ô√≠d√≠ postup model≈Ø linkou:

```python
class FactoryManager:
    def __init__(self, config):
        # Importuje ENV z entropy.core, ale logiku tr√©ninku m√° v factory.*
        from entropy.env import SwarmEnv
        from factory.stations import Kindergarten, Gym, ...
        
        self.stations = [
            Kindergarten(),
            Gym(),
            # ...
        ]
        self.registry = ModelRegistry()  # Sleduje hashe a rodokmeny
            LanguageSchoolStation(),
            TeamBuildingStation(),
            WarRoomStation()
        ]
        self.registry = ModelRegistry()  # Sleduje hashe a rodokmeny
        
    def process_model(self, model_id):
        current_checkpoint = self.registry.get_latest(model_id)
        
        for station in self.stations:
            print(f"üè≠ Factory: Entering {station.name}...")
            
            # 1. Warmup (Dry run)
            if not station.warmup(current_checkpoint):
                print(f"‚ùå Warmup failed at {station.name}. Aborting.")
                return False
                
            # 2. Train with 3 retries
            success = False
            for attempt in range(3):
                success, model = station.train(current_checkpoint)
                if success: break
                print(f"‚ö†Ô∏è Attempt {attempt+1} failed. Retrying...")
            
            if not success:
                print(f"üõë Station {station.name} FATAL FAIL. Discarding batch.")
                return False
                
            # 3. Quality Gate
            if not station.validate(model):
                print(f"üìâ QA Gate failed at {station.name}.")
                return False
                
            # 4. Save & Proceed
            current_checkpoint = self.registry.save(model, station.name)
            
        return True # Master Model Certified!
```

================================================================================
4. MODULARITA A WEIGHT FUSION (Stanice 3)
================================================================================

Kl√≠ƒçem k rychlosti je NEUƒåIT se v≈°e znovu. Na Stanici 3 pou≈æ√≠v√°me **Trainable Fusion Layer**:

```python
class Station3Net(nn.Module):
    def __init__(self, s1_comm_weights, s2_motor_weights):
        # 1. Naƒçti experty a ZAMRAZ je (Frozen)
        self.comm_expert = load_frozen(s1_comm_weights)
        self.motor_expert = load_frozen(s2_motor_weights)
        
        # 2. Tr√©nuj pouze F√∫zi (jak se maj√≠ ovliv≈àovat)
        self.fusion_head = nn.Sequential(
            nn.Linear(64 + 64, 128), 
            nn.ReLU(),
            nn.Linear(128, final_action_dim)
        )
```

================================================================================
4.2 JAX IMPLEMENTACE SKEPTICISMU
================================================================================

Fallible Expert v JAXu je velmi efektivn√≠ - error_rate je jen maska:

```python
@jax.jit
def skepticism_aware_loss(params, state, expert_action, is_expert_correct):
    """
    Loss funkce pro Skepticism Dial.
    Uƒç√≠ AI verifikovat rady m√≠sto slep√©ho n√°sledov√°n√≠.
    """
    # AI inference
    ai_action = network.apply(params, state)
    
    # Imitation loss (vzd√°lenost AI vs Expert)
    imitation_loss = jnp.sum((ai_action - expert_action) ** 2)
    
    # KL√çƒåOV√ù TRIK:
    # Pokud expert l≈æe (is_expert_correct=False), chceme MAXIMALIZOVAT rozd√≠l!
    # ‚Üí "Dƒõlej opak toho, co ≈ô√≠k√° lh√°≈ô"
    
    # V√°hov√°n√≠ loss podle correctness:
    # - Expert m√° pravdu (True) ‚Üí minimalizuj rozd√≠l (uƒçen√≠m)
    # - Expert l≈æe (False) ‚Üí ignoruj loss (nebo ji otoƒç)
    weighted_imitation = jnp.where(
        is_expert_correct, 
        imitation_loss,      # Uƒçen√≠ od spr√°vn√©ho experta
        0.0                  # Ignorov√°n√≠ ≈°patn√©ho experta
    )
    
    # RL loss (dosa≈æen√≠ c√≠le) funguje V≈ΩDY
    # ‚Üí I kdy≈æ expert l≈æe, AI se st√°le sna≈æ√≠ o c√≠l
    rl_loss = -jnp.sum(rewards)  # PPO objective
    
    return weighted_imitation + rl_loss
```

V√ùHODA V JAX:
  - `jnp.where()` je plnƒõ vektorizovan√©
  - Batch processing N agent≈Ø √ó M expert≈Ø najednou
  - JIT kompilace eliminuje overhead

================================================================================
5. PIPELINE PARALLELISM
================================================================================

Na jedn√© GPU (RTX 3060) lze linku "naplnit" v√≠ce modely v r≈Øzn√Ωch f√°z√≠ch:

VRAM Slot 1: Model C (Station 1 - 5% VRAM)
VRAM Slot 2: Model B (Station 2 - 15% VRAM)
VRAM Slot 3: Model A (Station 4 - 60% VRAM)

Factory Manager hl√≠d√° alokaci VRAM a spou≈°t√≠ stanice tak, aby nedo≈°lo k OOM (Out Of Memory).

================================================================================
6. KONFIGURACE (Hydra)
================================================================================

```yaml
# configs/factory/master_line.yaml

factory:
  pipeline: [kindergarten, gym, language_school, team_building, war_room]
  parallel_slots: 3
  auto_retry: 3
  
stations:
  kindergarten:
    target_accuracy: 0.99
    max_time: 10m
  gym:
    target_success_rate: 0.95
    expert_type: "astar"
  ...
```

================================================================================
7. KDY TO ROZB√çT? (Line Interrupt)
================================================================================

Factory Manager automaticky zastav√≠ linku, pokud:
1. Loss exploduje (NaN/Inf).
2. Accuracy stagnuje d√©le ne≈æ 20% vyhrazen√©ho ƒçasu stanice.
3. GPU teplota/power limit p≈ôekroƒç√≠ bezpeƒçnou mez.

================================================================================
                            KONEC FACTORY
                            
        Toto je posledn√≠ dokument Blueprint. V≈°e je p≈ôipraveno!
        Vra≈• se k 00_OVERVIEW.txt nebo zaƒçni implementaci!
================================================================================
