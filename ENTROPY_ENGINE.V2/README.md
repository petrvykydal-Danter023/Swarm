# ðŸŒ€ ENTROPY ENGINE V2

> **High-performance 2D Reinforcement Learning framework for training swarm behaviors on consumer hardware.**

[![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![Stable-Baselines3](https://img.shields.io/badge/SB3-RecurrentPPO-green.svg)](https://sb3-contrib.readthedocs.io/)
[![WandB](https://img.shields.io/badge/Logging-WandB-orange.svg)](https://wandb.ai/)

---

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ENTROPY ENGINE V2                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   core/     â”‚    env/     â”‚    training/    â”‚     shared/       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Physics  â”‚  â€¢ PettingZooâ”‚  â€¢ RecurrentPPO â”‚  â€¢ Pygame Render  â”‚
â”‚  â€¢ Entities â”‚  â€¢ Rewards   â”‚  â€¢ Multicore    â”‚  â€¢ Rich Logger    â”‚
â”‚  (Pymunk)   â”‚  â€¢ Sensors   â”‚  â€¢ Callbacks    â”‚  â€¢ WandB          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Module | Description |
|--------|-------------|
| **`core/`** | Physics engine (Pymunk) + Entity classes (Agent, Wall, Goal) |
| **`env/`** | PettingZoo-compliant ParallelEnv with Lidar sensors and rewards |
| **`training/`** | SB3 training scripts, multicore wrapper, callbacks |
| **`shared/`** | Pygame renderer, Rich console logger, WandB integration |

---

## ðŸ¤– The Agent

### Observations (36D Vector)
| Component | Dimensions | Description |
|-----------|------------|-------------|
| **Lidar** | 32 | Distance to obstacles in 360Â° (normalized) |
| **Velocity** | 2 | Current speed (x, y) |
| **Goal Vector** | 2 | Relative direction to target (ego-centric) |

### Actions (2D Continuous)
| Output | Range | Description |
|--------|-------|-------------|
| **Left Motor** | [-1, 1] | Power to left wheel |
| **Right Motor** | [-1, 1] | Power to right wheel |

### Brain (LSTM)
- **Architecture:** MlpLstmPolicy (256 hidden units)
- **Memory:** Agents remember past states for temporal reasoning

---

## ðŸš€ Training Pipeline

### Parallel Training (Multicore)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MAIN PROCESS                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           RecurrentPPO (GPU)                    â”‚   â”‚
â”‚  â”‚           Shared Neural Network                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â”‚                              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚    â–¼         â–¼          â–¼          â–¼         â–¼         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”        â”‚
â”‚ â”‚Env 1â”‚  â”‚Env 2â”‚   â”‚Env 3â”‚   â”‚ ... â”‚   â”‚Env 8â”‚        â”‚
â”‚ â”‚10 agâ”‚  â”‚10 agâ”‚   â”‚10 agâ”‚   â”‚     â”‚   â”‚10 agâ”‚        â”‚
â”‚ â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜        â”‚
â”‚   CPU      CPU       CPU       CPU       CPU           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         = 80 agents training in parallel
```

### CTDE Paradigm
> **Centralized Training, Decentralized Execution**

| Phase | Behavior |
|-------|----------|
| **Training** | All 80 agents share ONE neural network (parameter sharing) |
| **Inference** | Each agent runs independently with local observations only |

---

## ðŸ“Š Training Metrics Reference

| Metric | Description | Good Values |
|--------|-------------|-------------|
| `fps` | Environment steps per second | Higher = faster training |
| `loss` | Total loss (policy + value + entropy) | Should decrease |
| `value_loss` | Critic prediction error | Should decrease |
| `explained_variance` | How well Critic understands the environment | 0â†’1 (higher = better) |
| `entropy_loss` | Exploration encouragement | Gradually decreases |
| `approx_kl` | Policy change magnitude | < 0.02 (PPO constraint) |
| `clip_fraction` | Updates clipped by PPO | < 0.2 |
| `std` | Action randomness | Decreases as agent becomes confident |

---

## ðŸ› ï¸ Getting Started

### 1. Install Dependencies
```bash
pip install -r ENTROPY_ENGINE.V2/requirements.txt
```

### 2. (Optional) Setup WandB
```bash
wandb login
```

### 3. Run Training
```bash
# Single-core (10 agents)
python ENTROPY_ENGINE.V2/training/train_lstm.py

# Multi-core (80 agents, 8 processes)
python ENTROPY_ENGINE.V2/training/train_multicore.py
```

### 4. Monitor Progress
- **Console:** Rich progress bar + live FPS
- **WandB:** [wandb.ai/petr-vykydal/entropy-engine-v2](https://wandb.ai/petr-vykydal/entropy-engine-v2)
- **Local:** GIFs saved to `videos/`

---

## ðŸ“‚ Directory Structure

```
ENTROPY_ENGINE.V2/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ entities.py      # Agent, Wall, Goal classes
â”‚   â”œâ”€â”€ physics.py       # Pymunk world wrapper
â”‚   â””â”€â”€ world.py         # PhysicsWorld manager
â”œâ”€â”€ env/
â”‚   â””â”€â”€ entropy_env.py   # PettingZoo ParallelEnv
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_lstm.py    # Single-core training
â”‚   â”œâ”€â”€ train_multicore.py # 8-process parallel training
â”‚   â”œâ”€â”€ multicore_wrapper.py # AsyncVectorizedEntropyEnv
â”‚   â”œâ”€â”€ custom_wrapper.py # PettingZoo â†’ VecEnv adapter
â”‚   â””â”€â”€ callbacks.py     # GIF recording, Rich logging
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ rendering.py     # Pygame renderer
â”‚   â””â”€â”€ logger.py        # Rich console logger
â”œâ”€â”€ models/              # Saved .zip model checkpoints
â”œâ”€â”€ videos/              # Generated GIFs (start/end/comparison)
â”œâ”€â”€ runs/                # TensorBoard logs
â”œâ”€â”€ wandb/               # WandB run metadata
â””â”€â”€ requirements.txt
```

---

## ðŸ”§ Configuration

Key hyperparameters in `train_multicore.py`:

```python
N_ENVS = 8              # Parallel processes
AGENTS_PER_ENV = 10     # Agents per world
total_timesteps = 1_000_000
learning_rate = 3e-4
n_steps = 512
batch_size = 4096
lstm_hidden_size = 256
```

---

## ðŸ“ˆ Performance

| Metric | Value |
|--------|-------|
| **Training Speed** | ~600-1600 FPS |
| **Parallel Agents** | 80 |
| **GPU Utilization** | ~25% (bottlenecked by CPU physics) |
| **Time to 1M steps** | ~25-30 minutes |

---

## ðŸš§ Roadmap

- [ ] Numba-accelerated Lidar raycasting
- [ ] Shared memory IPC (replace Pipe)
- [ ] Inter-agent communication channels
- [ ] JAX/Brax GPU physics migration
- [ ] Curriculum learning stages

---

<p align="center">
  <b>Built with ðŸ§  by the Entropy Team</b>
</p>