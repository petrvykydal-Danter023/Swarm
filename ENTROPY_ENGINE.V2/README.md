# ğŸŒ€ ENTROPY_ENGINE.V2

**Entropy Engine V2** is a high-performance, 2D Reinforcement Learning (RL) framework designed for training complex swarm behaviors on consumer hardware. It combines accurate physics simulation with efficient parallel training pipelines.

---

## ğŸ— Architecture

The engine is built on four core pillars:

1.  **Core (`core/`)**:
    *   **Physics**: Powered by `Pymunk` (Chipmunk2D). Provides fast, stable rigid-body simulation with collision types, friction, and elasticity.
    *   **Entities**: Base classes for Agents, Walls, and Goals.
2.  **Environment (`env/`)**:
    *   **API**: `PettingZoo` compliant ParallelEnv (multi-agent standard).
    *   **Logic**: Handles resetting, stepping the physics world, sensor updates, and reward calculation.
3.  **Training (`training/`)**:
    *   **Library**: `Stable-Baselines3` (SB3).
    *   **Algorithm**: `RecurrentPPO` (PPO + LSTM). Agents have memory and "consciousness" of past states.
    *   **Vectorization**: Custom `PettingZooToVecEnv` wrapper enabling **Parameter Sharing** (one brain controls all 10+ agents).
4.  **Shared (`shared/`)**:
    *   **Visualization**: Custom `Pygame` renderer for video generation.
    *   **Logging**: `Rich` console output and `WandB` cloud integration.

---

## ğŸ¤– The Agent

The agents are autonomous entities designed for swarm intelligence tasks.

### ğŸ§  Inputs (Observations)
Each agent perceives the world through a 36-dimensional vector:
*   **Lidar (32 rays):** Distance to obstacles (Walls, other Agents) in a 360Â° circle.
*   **Velocity (2 values):** Current linear velocity (x, y).
*   **Goal Vector (2 values):** Relative vector pointing to their assigned target.

### ğŸ¦¾ Outputs (Actions)
*   **Differential Drive (2 values):** Continuous control signals [-1, 1] for Left and Right motor power.

### ğŸ§  Brain (LSTM)
*   Agents use a **Long Short-Term Memory (LSTM)** network.
*   This allows them to remember temporal patterns (e.g., "I was turning left a second ago", "The goal was behind that wall").

---

## ğŸŒ Simulation Environment

*   **World:** Continuous 2D space (800x600).
*   **Physics Step:** 60 Hz (consistent simulation stability).
*   **Goal:** Agents must navigate to their dynamically spawned green targets while avoiding collisions with walls (static) and each other (dynamic).
*   **Rewards:**
    *   `+` Moving closer to goal.
    *   `+10` Reaching goal (respawns immediately).
    *   `-` Time penalty (encourages speed).

---

## ğŸš€ Getting Started

### 1. Prerequisites
```bash
pip install -r ENTROPY_ENGINE.V2/requirements.txt
```

### 2. Login to Weights & Biases (Optional but Recommended)
For cloud logging and video usage:
```bash
wandb login
```

### 3. Run Training
Train a swarm of 10 agents with LSTM memory:
```bash
python ENTROPY_ENGINE.V2/training/train_lstm.py
```

### 4. Monitor
*   **Console:** Live metrics (FPS, Reward) via `Rich`.
*   **WandB:** Graphs and "Before vs After" GIFs at [wandb.ai](https://wandb.ai).
*   **Local:** Videos saved in `./videos/`.

---

## ğŸ“‚ Directory Structure

```
ENTROPY_ENGINE.V2/
â”œâ”€â”€ core/           # Physics & Entities (Agent, Wall)
â”œâ”€â”€ env/            # PettingZoo Environment Logic
â”œâ”€â”€ training/       # SB3 Scripts & Callbacks
â”œâ”€â”€ shared/         # Rendering & Logging Utils
â””â”€â”€ requirements.txt
```
