
"""
Entropy Engine V3 - Universal Configuration System
Defines the hierarchy of configuration objects for the entire engine.
"""
from dataclasses import dataclass, field
from typing import Dict, Optional

@dataclass
class SimConfig:
    """
    Nastaven√≠ fyzik√°ln√≠ho svƒõta a simulace.
    """
    # Rozmƒõry ar√©ny (jednotky)
    arena_width: float = 800.0  
    arena_height: float = 600.0
    
    # D√©lka jedn√© epizody (poƒçet krok≈Ø)
    # 200 krok≈Ø @ 60 FPS = 3.3 sekundy re√°ln√©ho ƒçasu simulace (ne tr√©ninku)
    max_steps: int = 200  
    
    # Poƒçet paraleln√≠ch prost≈ôed√≠ pro tr√©nink (Batch Size)
    # 1 = Single Debug (train_fast_swarm)
    # 64+ = Massive Parallel (train_massive_swarm) - Doporuƒçeno pro stabilitu
    num_envs: int = 64  
    
    # Fyzik√°ln√≠ parametry (t≈ôen√≠, kolize...) lze p≈ôidat zde
    dt: float = 0.1

@dataclass
class CommConfig:
    """
    Pokroƒçil√° komunikace (Coordinated Unit).
    """
    mode: str = "spatial"  # "spatial" nebo "broadcast" (legacy)
    msg_dim: int = 16      # Velikost zpr√°vy (vektor)
    
    # --- Spatial & Attention ---
    max_neighbors: int = 5 # Top-K soused≈Ø pro Attention/Inbox
    
    # --- Dual-Channel Attention ---
    dual_attention: bool = True     # Use Local/Global split
    local_radius: float = 300.0     # Radius for "Local" (Tactical) messages
    local_heads: int = 2
    global_heads: int = 2
    
    # --- Event-Triggered Communication ---
    surprise_gating: bool = True    # Use Surprise to open Gate
    surprise_threshold: float = 0.1 # Min prediction error to allow speaking
    info_gain_reward: float = 0.1   # Reward for useful messages
    
    # --- Virtual Pheromones ---
    pheromones_enabled: bool = True # Enable Stigmergy
    pheromone_radius: float = 50.0  # Detection/Influence range
    pheromone_ttl: int = 100        # Time-to-live in steps
    max_pheromones: int = 100       # Buffer size (max active markers)
    pheromone_dim: int = 8          # Dimension of pheromone message
    
    # --- Dynamic Hierarchy ---
    hierarchy_enabled: bool = True     # Use Squads & Leaders
    squad_size: int = 5                # Target agents per squad
    leader_election_mode: str = "proximity" # "proximity", "random"
    leader_broadcast_only: bool = True # Restrict broadcast to leaders
    
    # --- Gating & Penalty ---
    gating_threshold: float = 0.5  # Sigmoid > 0.5 => Speak
    spam_penalty: float = -0.01    # Adaptivn√≠ penalizace za mluven√≠
    comm_warmup_epochs: int = 1000 # Epochy zdarma (bez penalizace)

@dataclass
class AgentConfig:
    """
    Nastaven√≠ agent≈Ø a jejich schopnost√≠.
    """
    # Poƒçet agent≈Ø v jedn√© ar√©nƒõ
    num_agents: int = 20
    
    # --- Senzory ---
    lidar_rays: int = 32
    lidar_range: float = 200.0
    
    # --- Komunikace ---
    # Pokud use_communication=True, pou≈æije se CommConfig n√≠≈æe
    use_communication: bool = False
    vocab_size: int = 4 # Legacy (pro zpƒõtnou kompatibilitu, pokud mode!=spatial)
    context_dim: int = 64 # Legacy
    
    # Nov√° konfigurace pro Spatial comms
    comm: CommConfig = field(default_factory=CommConfig)

@dataclass
class RewardConfig:
    """
    Nastaven√≠ V√°h Odmƒõn (Reward Shaping).
    Urƒçuje, co je pro agenty "dobr√©" a "≈°patn√©".
    """
    # 1. Vzd√°lenost k c√≠li (Dense Reward)
    # Motivuje k pohybu smƒõrem k c√≠li.
    # Negativn√≠ hodnota = penalizace za vzd√°lenost (chce b√Ωt bl√≠zko = 0)
    w_dist: float = 1.0  
    
    # 2. Dosa≈æen√≠ c√≠le (Sparse Reward)
    # Velk√Ω bonus za dotknut√≠ se c√≠le.
    w_reach: float = 10.0
    
    # 3. Penalizace za energii (Motor usage)
    # Motivuje k efektivn√≠mu pohybu (nepl√Ωtvat palivem).
    # Z√°porn√° hodnota.
    w_energy: float = -0.01 
    
    # 4. Living Penalty (Time pressure)
    # Mal√° penalizace za ka≈æd√Ω krok, kdy agent NEN√ç v c√≠li.
    # Zabra≈àuje strategii "st≈Øj a ≈°et≈ôi energii".
    w_living_penalty: float = -0.001
    
    # 5. Sd√≠len√≠ c√≠le (Shared Goal)
    # True = V≈°ichni agenti maj√≠ jeden spoleƒçn√Ω c√≠l (Flocking).
    # False = Ka≈æd√Ω m√° sv≈Øj unik√°tn√≠ c√≠l (Routing/Traffic).
    shared_goal: bool = False

@dataclass
class PPOConfig:
    """
    Hyperparametry pro MAPPO (Multi-Agent PPO).
    """
    # Learning Rate pro Actora (Pohyb/Mluven√≠)
    lr_actor: float = 3e-4  
    
    # Learning Rate pro Critica (Odhad hodnoty)
    # Obvykle vy≈°≈°√≠ ne≈æ actor, aby se rychleji stabilizoval.
    lr_critic: float = 1e-3 
    
    # Poƒçet aktualizac√≠ s√≠tƒõ na jeden batch dat
    actor_updates: int = 4
    critic_updates: int = 1
    
    # Clip Range pro PPO (jak moc se m≈Ø≈æe zmƒõnit strategie v jednom kroku)
    clip_eps: float = 0.2
    
    # Gamma (Discount Factor) - jak moc z√°le≈æ√≠ na budoucnosti
    gamma: float = 0.99

@dataclass
class HogConfig:
    """
    Hand of God (HOG) - Expertn√≠ Asistence
    """
    enabled: bool = True
    start_weight: float = 1.0 
    end_weight: float = 0.0
    decay_epochs: int = 5000 
    
    # Adaptivn√≠ m√≥d
    # Pokud True, decay_epochs se ignoruje a pomoc kles√°, 
    # jen kdy≈æ agent dos√°hne target_reward.
    adaptive: bool = False
    target_reward: float = -0.1 # Nula je perfektn√≠ (b√Ωt na c√≠li)

@dataclass
class RenderConfig:
    """
    Nastaven√≠ vizualizace a ukl√°d√°n√≠.
    """
    enabled: bool = True
    render_every: int = 1000
    fps: int = 20
    output_dir: str = "outputs/universal_experiment"

@dataclass(unsafe_hash=True)
class IntentConfig:
    """
    Konfigurace Intent-Based Actions (Phase 2).
    """
    enabled: bool = False             # Pokud False, pou≈æ√≠v√° se Direct Action (Motor L/R)
    
    # PID parametry pro p≈ôevod Target -> Motor
    pid_pos_kp: float = 2.0
    pid_pos_kd: float = 0.5
    pid_rot_kp: float = 5.0
    pid_rot_kd: float = 0.5
    
    # Limity
    max_linear_accel: float = 5.0
    max_angular_accel: float = 10.0

@dataclass(unsafe_hash=True)
class SafetyConfig:
    """
    Konfigurace Safety Layer (Reflexy).
    """
    enabled: bool = True
    
    # === Collision Avoidance ===
    safety_radius: float = 30.0        # Start slowing down at this distance
    min_distance: float = 10.0         # Hard stop distance
    collision_check_radius: float = 60.0 # Only check agents within this radius (scaling)
    
    # === Repulsion (Liquid Swarm) ===
    enable_repulsion: bool = True
    repulsion_radius: float = 25.0     # Start repelling at this distance
    repulsion_force: float = 0.5       # Strength of push
    
    # === Speed Limits ===
    max_speed: float = 10.0            # Absolute max velocity
    emergency_brake_dist: float = 5.0  # Hard brake distance
    
    # === Communication Limits ===
    msg_rate_limit: int = 5            # Max messages per N steps
    msg_rate_window: int = 10          # Window size in steps
    
    # === Energy Management ===
    energy_enabled: bool = False  # Toggle
    low_battery_threshold: float = 0.2     # 20% - reduce speed
    critical_battery_threshold: float = 0.05  # 5% - force return
    low_battery_speed_mult: float = 0.5
    
    # === Watchdog (Anti-Stalemate) ===
    watchdog_enabled: bool = True  # Toggle
    stalemate_window: int = 100        # Check every N steps
    stalemate_min_distance: float = 5.0  # Must move at least this far
    stalemate_random_duration: int = 20  # Random walk duration
    stalemate_random_speed: float = 0.5
    
    # === Geo-Fence ===
    geofence_enabled: bool = True  # Toggle
    geofence_push_distance: float = 30.0
    geofence_push_force: float = 1.0
    
    # === Override ===
    allow_ai_override: bool = True     # Can AI disable reflexes?
    
    # === Metrics ===
    log_metrics: bool = True
    log_interval: int = 100   # Log every N steps

@dataclass
class ExperimentConfig:
    """
    MASTER CONFIG - Ko≈ôenov√Ω objekt pro cel√Ω experiment.
    """
    name: str = "default_experiment"
    total_epochs: int = 50000 
    
    # Resume / Transfer Learning
    # Cesta k .pkl souboru s checkpointem. Pokud None, jede od nuly.
    load_checkpoint: Optional[str] = None
    
    sim: SimConfig = field(default_factory=SimConfig)
    agent: AgentConfig = field(default_factory=AgentConfig)
    reward: RewardConfig = field(default_factory=RewardConfig)
    ppo: PPOConfig = field(default_factory=PPOConfig)
    hog: HogConfig = field(default_factory=HogConfig)
    render: RenderConfig = field(default_factory=RenderConfig)
    safety: SafetyConfig = field(default_factory=SafetyConfig)
    intent: IntentConfig = field(default_factory=IntentConfig)


# =============================================================================
# üì¶ ENTROPY ENGINE V3 - INVENTORY & CAPABILITIES
# =============================================================================
# Zde je seznam v≈°eho, co je simulovateln√© a podporovan√© v aktu√°ln√≠ verzi Enginu.
#
# 1. OBJEKTY A ENTITY
#    - [x] Agent (Circle): M√° pozici, rotaci, rychlost, barvu (dle t√Ωmu/stavu).
#    - [x] C√≠l (Target): Bod v prostoru (kruh), statick√Ω nebo dynamick√Ω (pohybliv√Ω).
#    - [x] P≈ôek√°≈æky (Obstacles):
#          - [x] Hranice ar√©ny (Walls): Pevn√© zdi, odr√°≈æ√≠ agenty.
#          - [ ] Vnit≈ôn√≠ objekty (Boxy/Kruhy): Zat√≠m statick√© v k√≥du, lze p≈ôidat do configu.
#    - [ ] Z√≥ny (Zones): Oblasti s jin√Ωm t≈ôen√≠m nebo speci√°ln√≠m efektem (Damage/Heal).
#
# 2. SENZORY (VSTUPY)
#    - [x] LIDAR: Paprskov√Ω senzor detekuj√≠c√≠ vzd√°lenost k p≈ôek√°≈æk√°m/agent≈Øm.
#    - [x] Relativn√≠ Pozice C√≠le (GPS): Vektor k c√≠li [dx, dy].
#    - [x] Rychlost (Velocity): Vlastn√≠ vektor pohybu [vx, vy].
#    - [x] Inbox (Spatial Comms): P≈ô√≠jem zpr√°v od Top-K soused≈Ø + Metadata (Angle, Dist).
#    - [ ] Viz√°ln√≠ Vstup (Pixel-based): Renderovan√Ω pohled (p≈ô√≠li≈° pomal√© pro miliony krok≈Ø, nepou≈æ√≠v√°me).
#
# 3. AKCE (V√ùSTUPY)
#    - [x] Pohyb (Continuous): Tank-drive [Lev√Ω_Motor, Prav√Ω_Motor] nebo [Speed, Rotate].
#    - [x] Komunikace (Complex): 
#          - Broadcast: V≈°esmƒõrov√© vys√≠l√°n√≠.
#          - Spatial: C√≠len√© vys√≠l√°n√≠ na sou≈ôadnice (Angle/Dist).
#          - Gating: Mo≈ænost mlƒçet (≈°et≈ô√≠ penalizaci).
#    - [ ] Manipulace: Chyt√°n√≠ objekt≈Ø (Gripper) - pl√°nov√°no pro V4.
#
# 4. FYZIKA
#    - [x] Kinematika: Newtonovsk√Ω pohyb, setrvaƒçnost.
#    - [x] Kolize: Pru≈æn√© sr√°≈æky (Agent-Agent, Agent-Zeƒè).
#    - [x] T≈ôen√≠ (Friction): Line√°rn√≠ zpomalov√°n√≠.
#    - [x] Energie: Spot≈ôeba paliva dle v√Ωkonu motor≈Ø.
#
# 5. ML & TR√âNINK
#    - [x] algoritmus: MAPPO (Multi-Agent PPO) s CTDE architekturou.
#    - [x] Pamƒõ≈•: GRU (Recurrent Actor) pro udr≈æen√≠ kontextu (Telepatie).
#    - [x] Hand of God: Expertn√≠ navigace (vektorov√° pole) pro guiding.
#    - [x] Curriculum: Postupn√© ztƒõ≈æov√°n√≠ (HOG decay, Spam penalty ramp-up).
#    - [x] Massive Parallelism: JAX VMAP (64+ vesm√≠r≈Ø nar√°z).
#    - [x] Checkpointing: Ukl√°d√°n√≠/Naƒç√≠t√°n√≠ stavu s√≠tƒõ a optimiz√©ru.
#
# 6. VIZUALIZACE (RENDERER)
#    - [x] Headless: Bƒõ≈æ√≠ na serveru bez monitoru.
#    - [x] Elementy: Agenti (≈°ipky smƒõru), C√≠le (teƒçky), Lidar (paprsky), Historie (stopy).
#    - [x] V√Ωstup: GIF animace, MP4 video (p≈ôes imageio).
#    - [ ] Real-time GUI: Okno s ovl√°d√°n√≠m my≈°√≠ (nepodporov√°no v massive m√≥du).
#
# 7. TYPY √öLOH (SC√âN√Å≈òE)
#    - [x] Navigace (Routing): Ka≈æd√Ω agent m√° sv≈Øj c√≠l.
#    - [x] Shlukov√°n√≠ (Flocking): V≈°ichni maj√≠ jeden c√≠l.
#    - [ ] Pron√°sledov√°n√≠ (Tag): T√Ωm A hon√≠ T√Ωm B.
#    - [ ] Fotbal/Tlaƒçen√≠: Manipulace s pasivn√≠m objektem.
#
# 8. SAFETY LAYER (Hybrid Architecture)
#    - [x] Collision Reflex: Automatick√© zpomalen√≠ u p≈ôek√°≈æek (squad-aware).
#    - [x] Agent Repulsion: Odpuzov√°n√≠ agent≈Ø mimo vlastn√≠ squad (liquid swarm).
#    - [x] Geo-Fence: Virtu√°ln√≠ s√≠la od hranic ar√©ny.
#    - [x] Token Bucket Comm Limiter: Anti-spam s rozlo≈æen√Ωm refillem.
#    - [x] Watchdog (Anti-Stalemate): Detekce zacyklen√≠ + n√°hodn√Ω √∫tƒõk.
#    - [x] Safety Metrics: Telemetrie intervenc√≠ (speed_reductions, hard_stops).
#    - [ ] AI Override: Mo≈ænost AI p≈ôepsat safety (action space nepodporuje).
#    - [ ] Energy Governor: Zpomalen√≠ p≈ôi n√≠zk√© energii (logika neaktivn√≠).
#
# 9. INTENT SYSTEM (High-Level Control)
#    - [x] Velocity Mode: Vstup [v, omega] ‚Üí v√Ωstup [Motor_L, Motor_R].
#    - [x] Target Mode: Vstup [rel_x, rel_y] ‚Üí PID kontroler ‚Üí motory.
#    - [x] IntentConfig: PID parametry (pid_pos_kp, pid_rot_kp, atd.).
#    - [ ] Follow Intent: Sledov√°n√≠ jin√©ho agenta (nutn√Ω extra k√≥d).
#    - [ ] Formation Intent: Pozice ve formaci (nutn√° squad logika).
# =============================================================================


# =============================================================================
# üìä FINDINGS (Benchmark Results)
# =============================================================================
# Key insights from experiments comparing control architectures:
#
# BENCHMARK: Heuristic "Go To Goal" policy, 200 steps, 20 agents
# 
# | Mode                        | Reward   | Goals Reached | FPS  |
# |-----------------------------|----------|---------------|------|
# | Direct (Unsafe)             | -73      | 0             | 124  |
# | Direct + Safety             | -73      | 0             | 30   |
# | Hybrid (Intent + Safety)    | +1252    | 2541          | 28   |
#
# CONCLUSION:
# - Hybrid architecture is the CLEAR WINNER (2541 goals vs 0).
# - Direct control cannot properly translate direction vectors to 
#   differential drive (Motor L/R) without the Intent Translator's PID.
# - Safety Layer is working correctly (26 interventions in Hybrid mode).
# - FPS drop (~4x) is acceptable trade-off for massive goal achievement.
#
# RECOMMENDATION: Always use Hybrid mode (intent.enabled=True, safety.enabled=True)
# for any real training scenario.
# =============================================================================
