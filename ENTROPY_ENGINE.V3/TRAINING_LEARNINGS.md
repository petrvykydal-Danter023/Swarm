# ğŸ§  Entropy Engine V3 - Training Tips & Learnings

Tento dokument shrnuje klÃ­ÄovÃ© poznatky zÃ­skanÃ© bÄ›hem vÃ½voje a optimalizace trÃ©ninkovÃ©ho procesu Swarm Intelligence.

## 1. Massive Parallelism (VelkÃ© mnoÅ¾stvÃ­ prostÅ™edÃ­) ğŸŒ
*   **Co to je**: SpuÅ¡tÄ›nÃ­ desÃ­tek aÅ¾ stovek simulacÃ­ (Envs) najednou pomocÃ­ `jax.vmap`.
*   **ProÄ to funguje**: PPO (Proximal Policy Optimization) je "on-policy" algoritmus, kterÃ½ je velmi citlivÃ½ na "Å¡um" v datech.
*   **Efekt**:
    *   KdyÅ¾ bÄ›Å¾Ã­ **1 prostÅ™edÃ­** (20 agentÅ¯): Gradient je nestabilnÃ­, agenti se mohou "zacyklit" ve Å¡patnÃ© strategii.
    *   KdyÅ¾ bÄ›Å¾Ã­ **64 prostÅ™edÃ­** (1280 agentÅ¯): PrÅ¯mÄ›rnÃ½ gradient je extrÃ©mnÄ› pÅ™esnÃ½. SÃ­Å¥ se uÄÃ­ robustnÃ­ chovÃ¡nÃ­, protoÅ¾e vidÃ­ "vÅ¡echny moÅ¾nÃ© situace" v kaÅ¾dÃ©m kroku.
*   **Tip**: VÅ¾dy se snaÅ¾te maximalizovat poÄet prostÅ™edÃ­, co  vÃ¡m pamÄ›Å¥ GPU/CPU dovolÃ­. VÃ­ce agentÅ¯ = stabilnÄ›jÅ¡Ã­ a rychlejÅ¡Ã­ konvergence (na poÄet iteracÃ­).

## 2. Hand of God (HOG) - Curriculum Learning ğŸ‘»
*   **ProblÃ©m**: Na zaÄÃ¡tku je sÃ­Å¥ nÃ¡hodnÄ› inicializovanÃ¡. Agenti se motajÃ­ v kruhu a trvÃ¡ dlouho, neÅ¾ nÃ¡hodou narazÃ­ na cÃ­l a dostanou odmÄ›nu.
*   **Å˜eÅ¡enÃ­**: Vnutit jim experimentÃ¡lnÄ› "sprÃ¡vnÃ½ smÄ›r" (Expert Vector) na zaÄÃ¡tku trÃ©ninku.
*   **Implementace**: LineÃ¡rnÃ­ decay (100% pomoc -> 0% pomoc).
*   **VÃ½sledek**: Agenti okamÅ¾itÄ› "ochutnajÃ­" odmÄ›nu. Critic se rychle nauÄÃ­, Å¾e "bÃ½t u cÃ­le je dobrÃ©". Actor se pak snaÅ¾Ã­ tento stav zreprodukovat, i kdyÅ¾ pomoc slÃ¡bne.
*   **Tip**: Pokud se agenti neuÄÃ­, zkuste jim prvnÃ­ch 5-10% trÃ©ninku "vodit ruku".

## 3. Shared vs. Unique Goals ğŸ¯
*   **Unique Goals (Standard)**: KaÅ¾dÃ½ agent mÃ¡ svÅ¯j vlastnÃ­ cÃ­l.
    *   *VÃ½hoda*: Agenti jsou samostatnÃ­ a robustnÃ­.
    *   *NevÃ½hoda*: ObtÃ­Å¾nÃ© uÄenÃ­, agenti se navzÃ¡jem pletou ("kÅ™iÅ¾ovatka").
*   **Shared Goal (ZjednoduÅ¡enÃ­)**: VÅ¡ichni majÃ­ jeden spoleÄnÃ½ cÃ­l.
    *   *VÃ½hoda*: Ãšloha se mÄ›nÃ­ na "shlukovÃ¡nÃ­" (Flocking). SnadnÄ›jÅ¡Ã­ uÄenÃ­, mÃ©nÄ› kolizÃ­, moÅ¾nost kopÃ­rovat souseda.
    *   *NevÃ½hoda*: Riziko "stÃ¡dnÃ­ho efektu" (agent bez sousedÅ¯ je ztracen).
*   **Tip**: Pro rychlÃ½ debug navigace pouÅ¾ijte Shared Goal. Pro finÃ¡lnÃ­ "inteligentnÃ­" roj pouÅ¾ijte Unique Goals nebo Curriculum (nejdÅ™Ã­v Shared, pak Unique).

## 4. CTDE Architektura (Cooperation) ğŸ¤
*   **Centralized Training (Critic)**: Kritik vidÃ­ **celÃ½ stav svÄ›ta** (vÅ¡echny pozice). DÃ­ky tomu vÃ­, zda je situace dobrÃ¡ pro tÃ½m jako celek.
*   **Decentralized Execution (Actor)**: Agent (vojÃ¡k) vidÃ­ jen **lokÃ¡lnÃ­ okolÃ­** (Lidar). MusÃ­ se rozhodovat sÃ¡m.
*   **ProÄ to funguje**: BÄ›hem trÃ©ninku "BÅ¯h" (Critic) radÃ­ vojÃ¡kovi (Actor), co by mÄ›l udÄ›lat, aby pomohl tÃ½mu, i kdyÅ¾ vojÃ¡k nevidÃ­ celÃ½ obraz. Po trÃ©ninku uÅ¾ vojÃ¡k jednÃ¡ sÃ¡m, ale mÃ¡ v sobÄ› "intuici" vÅ¡tÃ­penou kritikem.

## 5. Rychlost je vÅ¡echno (JAX Scan + JIT) âš¡
*   **Python Loop**: PomalÃ½ (cca 60 FPS). Nutnost komunikace CPU <-> GPU v kaÅ¾dÃ©m kroku.
*   **JAX Scan**: CelÃ¡ epizoda (200 krokÅ¯) se zkompiluje do jednÃ© operace na GPU (XLA). Å½Ã¡dnÃ½ Python overhead.
*   **VÃ½sledek**: ZrychlenÃ­ 10x aÅ¾ 100x (1000+ FPS). UmoÅ¾Åˆuje trÃ©novat miliony krokÅ¯ za minuty.

## 6. Komunikace ğŸ—£ï¸
*   **Cena**: PÅ™idÃ¡nÃ­ komunikaÄnÃ­ch kanÃ¡lÅ¯ (vocab) zvÄ›tÅ¡uje vÃ½stupnÃ­ prostor sÃ­tÄ›. TrÃ©nink je pomalejÅ¡Ã­ a nÃ¡roÄnÄ›jÅ¡Ã­ na stabilitu.
*   **Tip**: NejdÅ™Ã­v nauÄte agenty chodit (pohyb only). AÅ¾ to umÃ­ perfektnÄ›, pÅ™idejte "Å™eÄ". UÄit se chodit a mluvit narÃ¡z je pro RL velmi tÄ›Å¾kÃ©.
