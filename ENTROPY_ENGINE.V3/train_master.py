
"""
Entropy Engine V3 - MASTER TRAINER
Universal training script that supports:
- Single & Massive Parallel modes
- Dynamic Configuration (Sim, Agent, Reward, PPO, HOG)
- Hand of God (Curriculum)
- Validation Rendering
"""
import jax
import jax.numpy as jnp
import numpy as np
import time
import sys
import os
import logging
import distrax
from functools import partial

# Ensure imports
sys.path.insert(0, os.getcwd())

from entropy.training.env_wrapper import EntropyGymWrapper
from entropy.training.mappo import OptimizedMAPPO
from entropy.visuals.recorder import Recorder
from entropy.training.checkpoint import CheckpointManager
from entropy.config import ExperimentConfig

# Logger
logger = logging.getLogger("MasterTrainer")
logging.basicConfig(level=logging.INFO)

def run_experiment(cfg: ExperimentConfig):
    print(f"ðŸš€ Starting Universal Experiment: {cfg.name}")
    print(f"ðŸ“„ Configuration:")
    print(f"   ðŸŒ Envs: {cfg.sim.num_envs} | Agents/Env: {cfg.agent.num_agents}")
    print(f"   ðŸ§  PPO: Actor={cfg.ppo.lr_actor} | Critic={cfg.ppo.lr_critic}")
    print(f"   ðŸ‘» HOG: {cfg.hog.enabled} (Decay: {cfg.hog.decay_epochs} eps)")
    print(f"   ðŸŽ¯ Rewards: Dist={cfg.reward.w_dist}, Reach={cfg.reward.w_reach}")
    
    # 0. Setup Recorder
    recorder = None
    if cfg.render.enabled:
        recorder = Recorder(output_dir=cfg.render.output_dir, fps=cfg.render.fps)

    # 1. Initialize Wrapper (using Universal Config)
    # Note: env_wrapper checks if cfg has 'sim' attribute to detect universal config
    env_wrapper = EntropyGymWrapper(cfg)
    
    # 2. Setup Learner
    print("ðŸ§  Initializing MAPPO Agent...")
    mappo_config = {
        "lr_actor": cfg.ppo.lr_actor,
        "lr_critic": cfg.ppo.lr_critic,
        "actor_updates_per_step": cfg.ppo.actor_updates,
        "critic_updates_per_step": cfg.ppo.critic_updates,
        "clip_eps": cfg.ppo.clip_eps,
        "agent": cfg.agent, # Pass Agent Config for Comms/Surprise Gating
        # "gamma": cfg.ppo.gamma  # Not used in init, but in update logic usually
    }
    trainer = OptimizedMAPPO(mappo_config)
    
    # Init Random Keys
    rng = jax.random.PRNGKey(42)
    rng, init_rng = jax.random.split(rng)
    
    # Init One Instance to get Dims
    dummy_state, dummy_obs = env_wrapper.reset(init_rng)
    obs_dim = dummy_obs.shape[-1]
    action_dim = env_wrapper.action_dim
    print(f"ðŸ“Š Obs Dim: {obs_dim} | Action Dim: {action_dim}")
    
    trainer.init_states(obs_dim, action_dim, cfg.agent.num_agents, init_rng)
    
    ckpt_manager = CheckpointManager(f"{cfg.render.output_dir}_checkpoints", max_to_keep=2)

    # --- RESUME FROM CHECKPOINT ---
    start_epoch = 1
    if cfg.load_checkpoint:
        print(f"ðŸ“¥ Loading Checkpoint: {cfg.load_checkpoint}")
        try:
            loaded_data = CheckpointManager.load_checkpoint_static(cfg.load_checkpoint) # We'll need to static method or just import function
            # OR simpler:
            from entropy.training.checkpoint import load_checkpoint
            loaded_data = load_checkpoint(cfg.load_checkpoint)
            
            # Restore States
            # Structure: {'params': {'actor': ..., 'critic': ...}, 'opt_state': {'actor': ..., 'critic': ...}, 'step': ...}
            # We assume we save it this way (see Save logic below)
            
            if 'params' in loaded_data and 'actor' in loaded_data['params']:
                # Restore Actor
                trainer.actor_state = trainer.actor_state.replace(
                    params=loaded_data['params']['actor'],
                    opt_state=loaded_data['opt_state']['actor']
                )
                # Restore Critic
                trainer.critic_state = trainer.critic_state.replace(
                    params=loaded_data['params']['critic'],
                    opt_state=loaded_data['opt_state']['critic']
                )
                start_epoch = loaded_data.get('step', 0) + 1
                print(f"âœ… Resumed from Epoch {start_epoch}")
            else:
                 print("âš ï¸ Checkpoint format unrecognized. Starting from scratch.")
        except Exception as e:
            print(f"âŒ Failed to load checkpoint: {e}")

    # =========================================================================
    # LOGIC FUNCTIONS (JIT)
    # =========================================================================
    
    # ... (Logic functions rollout_scan_fn etc are same, omitted for brevity if unchanged logic)
    # But for 'replace_file_content' I must provide context or ensure I don't delete them.
    # The user asked for clean code, I should probably keep them. 
    # I am replacing lines 68-265 which INCLUDES logic functions.
    # I must RE-WRITE logic functions here.
    
    # Init Hidden States (Carries)
    # [B, N, Hidden]
    rng, init_carry_rng = jax.random.split(rng)
    init_actor_carries = jnp.zeros((cfg.sim.num_envs, cfg.agent.num_agents, trainer.hidden_dim))
    
    # =========================================================================
    # LOGIC FUNCTIONS (JIT)
    # =========================================================================

    # --- Expert Helper ---
    def get_expert_action(state):
        diff = state.goal_positions - state.agent_positions
        dist = jnp.linalg.norm(diff, axis=1, keepdims=True) + 1e-6
        direction = diff / dist
        expert_vec = direction * 1.0 # Speed
        
        # Padding for diverse actions (Gate, Channel, Angle, Dist, Msg...)
        # Action dim is now dynamic (e.g. 2 + 4 + MsgDim)
        # We only really care about the motor part for expert blending
        if action_dim > 2:
            padding = jnp.zeros((state.agent_positions.shape[0], action_dim - 2))
            return jnp.concatenate([expert_vec, padding], axis=-1)
        else:
            return expert_vec

    # --- Rollout Step ---
    def rollout_scan_fn(carry, x):
        # Unpack Extended Carry: (state, obs, actor_carry, last_pred_obs, rng, hog_weight)
        state, obs, actor_carry, last_pred_obs, rng, hog_weight = carry
        rng, a_key, s_key = jax.random.split(rng, 3)
        
        # 1. Inference (Recurrent)
        new_actor_carry, mean, log_std = trainer.actor_state.apply_fn(trainer.actor_state.params, obs, actor_carry)
        
        noise = jax.random.normal(a_key, mean.shape) * jnp.exp(log_std)
        agent_action = mean + noise
        
        # --- EVENT-TRIGGERED GATING (Surprise) ---
        if trainer.wm_state:
            from entropy.brain.world_model import compute_surprise
            
            # Compute Surprise: |Predicted_Obs - Actual_Obs|
            surprise = compute_surprise(last_pred_obs, obs) # [N, 1]
            
            # If Surprise < Threshold -> Silence (Gate = -10.0 or force 0)
            threshold = cfg.agent.comm.surprise_threshold
            should_speak = (surprise > threshold).astype(jnp.float32)
            
            # Modulate Action Gate (Index 2 in [Motor, Gate, ...])
            # Action: [Motor(2), Gate(1), ...]
            # We enforce Gate = -10 if should_speak is 0
            # agent_action[:, 2] = ... is hard in JAX arrays
            
            gate_col = agent_action[:, 2:3]
            # If speak: keep original gate. If silent: force -10.
            # But "Original Gate" might be "Don't Speak".
            # Standard logic: Surprise allows speaking, but Agent ultimately decides?
            # Or Surprise FORCES speaking?
            # User requirement: "Agent speaks ONLY IF surprised".
            # So: Effective Gate = Min(AgentGate, SurpriseGate).
            # If Surprise=0, we must force Gate to be low (below 0).
            
            # Soft gating:
            forced_silence = -10.0
            new_gate = should_speak * gate_col + (1.0 - should_speak) * forced_silence
            
            agent_action = agent_action.at[:, 2:3].set(new_gate)
            
            # Predict Next Obs (for next step's surprise)
            pred_next_obs = trainer.wm_state.apply_fn(trainer.wm_state.params, obs, agent_action)
        else:
            pred_next_obs = jnp.zeros_like(obs) # Dummy
            
        # 2. HOG Logic
        if cfg.hog.enabled:
            expert_action = get_expert_action(state)
            final_action = (1.0 - hog_weight) * agent_action + hog_weight * expert_action
        else:
            final_action = agent_action
            
        # 3. Log Prob
        dist = distrax.MultivariateNormalDiag(mean, jnp.exp(log_std))
        log_probs = dist.log_prob(final_action)
        
        # 4. Critic
        global_state = obs.reshape(1, -1)
        values = trainer.critic_state.apply_fn(trainer.critic_state.params, global_state)
        
        # 5. Environment Step
        next_state, next_obs, rewards, dones, info = env_wrapper.step(state, final_action, s_key)
        
        # 6. Memory Reset
        dones_mask = dones[..., None].astype(jnp.float32)
        next_actor_carry = new_actor_carry * (1.0 - dones_mask)
        # Reset prediction too? If done, next obs is unexpected anyway (teleport).
        # We should reset prediction to zero or next_obs (perfect prediction) to avoid surprise spike at start?
        # Actually random init leads to high surprise at T=0, which is good (exploration).
        next_pred_obs = pred_next_obs * (1.0 - dones_mask)
        
        step_data = {
            'obs': obs,
            'actions': final_action,
            'rewards': rewards,
            'dones': dones.astype(jnp.float32), 
            'values': values.flatten(),
            'log_probs': log_probs,
            'actor_states': actor_carry
        }
        return (next_state, next_obs, next_actor_carry, next_pred_obs, rng, hog_weight), step_data

    # --- Massive Parallel Logic (VMAP) ---
    @jax.jit
    def run_epoch_pipeline(rng, states, obss, actor_carries, last_pred_obss, hog_weight):
        
        def single_env_rollout(state, obs, carry, lpo, r, h):
            carry_in = (state, obs, carry, lpo, r, h)
            carry_out, trajectory = jax.lax.scan(rollout_scan_fn, carry_in, None, length=cfg.sim.max_steps)
            
            last_state, last_obs, last_carry, last_pred, last_rng, _ = carry_out
            
            global_state = last_obs.reshape(1, -1)
            last_val = trainer.critic_state.apply_fn(trainer.critic_state.params, global_state)
            last_val_expanded = last_val.reshape(1, -1)
            
            values_full = jnp.concatenate([trajectory['values'], last_val_expanded], axis=0)
            return (last_state, last_obs, last_carry, last_pred, last_rng), trajectory, values_full

        rngs = jax.random.split(rng, cfg.sim.num_envs)
        
        (last_states, last_obss, last_carries, last_preds, last_rngs), trajectories, all_values = jax.vmap(
            single_env_rollout, in_axes=(0, 0, 0, 0, 0, None)
        )(states, obss, actor_carries, last_pred_obss, rngs, hog_weight)
        
        return (last_states, last_obss, last_carries, last_preds, last_rngs[0]), trajectories, all_values

    # =========================================================================
    # TRAINING LOOP
    # =========================================================================
    try:
        reset_rngs = jax.random.split(rng, cfg.sim.num_envs)
        states, obss = jax.vmap(env_wrapper.reset)(reset_rngs)
        
        # Initial carries
        actor_carries = init_actor_carries
        last_pred_obss = jnp.zeros((cfg.sim.num_envs, cfg.agent.num_agents, obs_dim)) # Initial prediction is zero (max surprise)
        
        print(f"âš¡ JIT Compiling Pipeline (Mode: {'MASSIVE' if cfg.sim.num_envs > 1 else 'SINGLE'})...")
        
        start_time = time.time()
        last_time = time.time()
        
        # Adaptive HOG State
        curr_hog = cfg.hog.start_weight if cfg.hog.enabled else 0.0
        
        for epoch in range(start_epoch, cfg.total_epochs + 1):
            
            # --- CURRICULUM SCHEDULER ---
            if cfg.hog.enabled:
                if not cfg.hog.adaptive:
                    if epoch < cfg.hog.decay_epochs:
                        progress = (epoch - 1) / cfg.hog.decay_epochs
                        curr_hog = cfg.hog.start_weight - progress * (cfg.hog.start_weight - cfg.hog.end_weight)
                    else:
                        curr_hog = cfg.hog.end_weight
            else:
                curr_hog = 0.0
            
            curr_hog_jax = jnp.array(curr_hog, dtype=jnp.float32)

            # --- ROLLOUT ---
            # Pass and return actor_carries AND last_pred_obss
            (states, obss, actor_carries, last_pred_obss, rng), trajs, all_values = run_epoch_pipeline(
                rng, states, obss, actor_carries, last_pred_obss, curr_hog_jax
            )
            
            # --- DATA PREP ---
            rew_np = np.array(trajs['rewards'])   
            val_np = np.array(all_values)         
            don_np = np.array(trajs['dones']) # [T, N]
            carries_np = np.array(trajs['actor_states'])     
            
            all_advantages = []
            for i in range(cfg.sim.num_envs):
                adv = trainer._calculate_gae(rew_np[i], val_np[i], don_np[i])
                all_advantages.append(adv)
            all_advantages = np.concatenate(all_advantages, axis=0)  
            
            def flatten_batch(x):
                s = x.shape
                return x.reshape((s[0] * s[1],) + s[2:])
            
            # --- UPDATE ---
            values_trimmed = val_np[:, :-1, :].reshape(-1, cfg.agent.num_agents) 
            targets = all_advantages + values_trimmed
            
            all_advantages = (all_advantages - all_advantages.mean()) / (all_advantages.std() + 1e-8)
            all_advantages = jnp.array(all_advantages)
            targets = jnp.array(targets)
            
            obs_all = jnp.array(flatten_batch(trajs['obs']))
            obs_flat = obs_all.reshape(obs_all.shape[0], -1)
            
            # Critic
            critic_loss_val = 0.0
            for _ in range(cfg.ppo.critic_updates):
                trainer.critic_state, loss, _ = trainer._train_critic(
                    trainer.critic_state, obs_flat, targets.flatten(), cfg.agent.num_agents
                )
                critic_loss_val += loss
             
            actions_all = jnp.array(flatten_batch(trajs['actions']))
            log_probs_all = jnp.array(flatten_batch(trajs['log_probs']))
            carries_all = jnp.array(flatten_batch(trajs['actor_states'])) # [Batch, N, Hidden]
            
            total_agent_samples = obs_all.shape[0] * cfg.agent.num_agents
            
            obs_flat_actor = obs_all.reshape(total_agent_samples, -1)
            actions_flat = actions_all.reshape(total_agent_samples, -1)
            log_probs_flat = log_probs_all.reshape(total_agent_samples)
            adv_flat = all_advantages.reshape(total_agent_samples)
            carries_flat = carries_all.reshape(total_agent_samples, -1)
            
            # Actor
            actor_loss_val = 0.0
            for _ in range(cfg.ppo.actor_updates):
                trainer.actor_state, loss, _ = trainer._train_actor(
                    trainer.actor_state, 
                    obs_flat_actor, 
                    actions_flat, 
                    log_probs_flat, 
                    adv_flat,
                    carries_flat # Pass Carries
                )
                actor_loss_val += loss

            # --- LOGGING & ADAPTIVE LOGIC ---
            mean_reward = float(np.mean(rew_np))
            if cfg.hog.enabled and cfg.hog.adaptive:
                if mean_reward > cfg.hog.target_reward:
                    curr_hog = max(cfg.hog.end_weight, curr_hog - 0.005)
            
            if epoch % 10 == 0:
                curr_time = time.time()
                delta_time = curr_time - last_time
                last_time = curr_time
                steps_done = 10 * cfg.sim.max_steps * cfg.sim.num_envs
                fps = steps_done / (delta_time + 1e-6)
                print(f"Ep {epoch} | HOG: {curr_hog:.2f} | R: {mean_reward:.2f} | FPS: {fps:.0f} | L: {actor_loss_val:.2f}/{critic_loss_val:.2f}")
                
            # --- CHECKPOINT SAVE ---
            # Save every 500 epochs or if requested
            if epoch % 500 == 0:
                # Custom structural save
                save_data_params = {'actor': trainer.actor_state.params, 'critic': trainer.critic_state.params}
                save_data_opt = {'actor': trainer.actor_state.opt_state, 'critic': trainer.critic_state.opt_state}
                ckpt_manager.save(
                    params=save_data_params,
                    opt_state=save_data_opt,
                    step=epoch,
                    metrics={'reward': mean_reward}
                )
            
            # --- RENDER ---
            if recorder and epoch % cfg.render.render_every == 0:
                print(f"ðŸŽ¥ Recording validation epoch {epoch}...")
                filename = f"epoch_{experiment_name}_{epoch:04d}.gif" if 'experiment_name' in locals() else f"epoch_{epoch:04d}.gif"
                try:
                    # Use standard recorder (runs single CPU env episode)
                    recorder.record_episode(
                        env_wrapper, trainer.actor_state, trainer.actor_state.params, filename
                    )
                    print(f"âœ… GIF saved: {filename}")
                except Exception as e:
                     print(f"Recording failed: {e}")
                     
    except KeyboardInterrupt:
        print("ðŸ›‘ Stopped by user.")
    finally:
        pass

if __name__ == "__main__":
    print("Please use a specific experiment script to run this Master Trainer.")
    print("Example: python MASTER_TEMPLATE.py")
