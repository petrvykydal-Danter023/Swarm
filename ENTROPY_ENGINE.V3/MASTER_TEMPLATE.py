
"""
Entropy Engine V3 - MASTER EXPERIMENT TEMPLATE (10/10 Setup)
============================================================
Tento soubor slouÅ¾Ã­ jako kompletnÃ­ Å¡ablona pro nastavenÃ­ experimentu.
VÅ¡echny parametry jsou zde na jednom mÃ­stÄ› s vysvÄ›tlujÃ­cÃ­mi komentÃ¡Å™i.

JAK POUÅ½ÃT:
1. ZkopÃ­rujte tento soubor (napÅ™. 'my_experiment_01.py').
2. Upravte parametry v sekci CONFIGURATION.
3. SpusÅ¥te: `python my_experiment_01.py`.
"""
import sys
import os
sys.path.insert(0, os.getcwd())

from entropy.config import (
    ExperimentConfig, 
    SimConfig, 
    AgentConfig, 
    RewardConfig, 
    PPOConfig, 
    HogConfig, 
    RenderConfig
)
from train_master import run_experiment

# =========================================================================
# ğŸ› ï¸ CONFIGURATION (NASTAVENÃ)
# =========================================================================

experiment = ExperimentConfig(
    name = "universal_test_run",  # JmÃ©no experimentu (pro logy/files)
    total_epochs = 10000,         # DÃ©lka trÃ©ninku
    
    # ğŸ“¥ RESUME (POKRAÄŒOVÃNÃ)
    # Cesta k .pkl souboru. Pokud None, jede od nuly.
    # NapÅ™.: "outputs/universal_test_checkpoints/best.pkl"
    load_checkpoint = None,       
    
    # ğŸŒ SIMULACE A PROSTÅ˜EDÃ
    sim = SimConfig(
        num_envs = 64,            # 1 = Debug (pomalÃ© uÄenÃ­), 64+ = Massive (stabilnÃ­, rychlÃ©)
        max_steps = 200,          # DÃ©lka epizody (kroky)
        arena_width = 800.0,
        arena_height = 600.0
    ),
    
    # ğŸ¤– AGENTI
    agent = AgentConfig(
        num_agents = 20,          # Agenti v jednÃ© arÃ©nÄ›
        lidar_rays = 32,
        
        # --- KOMUNIKACE ---
        use_communication = False, # âœ… Zapnout/Vypnout "Å™eÄ"
        vocab_size = 4,            # Kolik slov umÃ­ (pokud zapnuto)
        context_dim = 64           # PamÄ›Å¥ na zprÃ¡vy
    ),
    
    # ğŸ¯ ODMÄšNY (Co je dobrÃ© a co Å¡patnÃ©?)
    reward = RewardConfig(
        w_dist = 1.0,      # Motivace jÃ­t k cÃ­li (LineÃ¡rnÃ­)
        w_reach = 10.0,    # Bonus za dosaÅ¾enÃ­ cÃ­le (SkokovÃ¡)
        w_energy = -0.01,  # Penalizace za plÃ½tvÃ¡nÃ­ palivem
        
        # --- MÃ“D CÃLE ---
        shared_goal = False # âœ… False = KaÅ¾dÃ½ mÃ¡ jinÃ½ cÃ­l (TÄ›Å¾kÃ©)
                            # âœ… True = VÅ¡ichni majÃ­ jeden cÃ­l (LehkÃ©/Flocking)
    ),
    
    # ğŸ§  TRÃ‰NINK (PPO HYPERPARAMETRY)
    ppo = PPOConfig(
        lr_actor = 3e-4,   # Rychlost uÄenÃ­ pohybu
        lr_critic = 1e-3,  # Rychlost uÄenÃ­ hodnocenÃ­
        actor_updates = 4, # KolikrÃ¡t pÅ™eÅ¾vÃ½kat data
        critic_updates = 1
    ),
    
    # ğŸ‘» HAND OF GOD (EXPERTNÃ ASISTENCE)
    hog = HogConfig(
        enabled = True,        # âœ… Zapnout "pomocnÃ¡ koleÄka"?
        start_weight = 1.0,    # 100% pomoc na zaÄÃ¡tku
        end_weight = 0.0,      # 0% pomoc na konci
        decay_epochs = 2000,   # Jak rychle pomoc zmizÃ­ (Curriculum)
        
        # --- ADAPTIVNÃ MÃ“D ---
        adaptive = False,      # âœ… True = "ChytrÃ½" Ãºstup (jen kdyÅ¾ to agentovi jde)
        target_reward = -0.1   # CÃ­lovÃ¡ odmÄ›na, pÅ™i kterÃ© sniÅ¾ujeme pomoc
    ),
    
    # ğŸ¥ VIZUALIZACE (VIDEO)
    render = RenderConfig(
        enabled = True,            # Generovat GIFy?
        render_every = 1000,       # Jak Äasto (kaÅ¾dÃ½ch X epoch)
        output_dir = "outputs/universal_test"
    )
)

# =========================================================================
# ğŸš€ SPUÅ TÄšNÃ
# =========================================================================

if __name__ == "__main__":
    # SpustÃ­ univerzÃ¡lnÃ­ trÃ©ninkovou smyÄku s tÃ­mto nastavenÃ­m
    run_experiment(experiment)
