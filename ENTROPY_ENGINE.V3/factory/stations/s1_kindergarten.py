"""
Station 1: The Kindergarten
Goal: Teach agent the meaning of words (Grounding) via Behavior Cloning.
Method: Supervised Learning on Oracle Demos.
"""
import logging
import jax
import jax.numpy as jnp
import numpy as np
import optax
from flax.training.train_state import TrainState
from typing import Any, Tuple, Optional, List, Dict
from factory.stations.base import Station
from factory.storage.demos import DemoStorage
from entropy.training.network import ActorCritic

class KindergartenStation(Station):
    """
    STATION 1: THE KINDERGARTEN
    Goal: Teach the agent basic vocabulary grounding.
    Input: Oracle Demos
    Output: Pre-trained Policy (Behavior Cloning)
    """
    def __init__(self, config: Any):
        super().__init__(config)
        self.logger = logging.getLogger("Kindergarten")
        self.config = config if isinstance(config, dict) else {}
        self.target_accuracy = self.config.get("target_accuracy", 0.95) # R^2 or low loss
        self.max_epochs = self.config.get("max_epochs", 10)
        self.batch_size = self.config.get("batch_size", 256)
        self.learning_rate = float(self.config.get("learning_rate", 1e-3))
        self.storage = DemoStorage(self.config)

    def warmup(self, model: Optional[Any] = None) -> bool:
        self.logger.info("Kindergarten warmup: checking data availability...")
        # Check if Oracle data exists
        try:
            # We look for the batch file generated by Station 0
            # Note: The filename might need to be config-driven to match S0
            self.storage.load_trajectory("oracle_demos_batch")
            return True
        except FileNotFoundError:
            self.logger.error("âŒ Oracle demos not found. Run Station 0 first.")
            return False
        except Exception as e:
            self.logger.error(f"âŒ Error checking data: {e}")
            return False

    def train(self, model: Optional[Any] = None) -> Tuple[bool, Any]:
        self.logger.info("ðŸŽ’ Starting Kindergarten training (Behavior Cloning)...")
        
        # 1. Load and Process Data
        demos = self.storage.load_trajectory("oracle_demos_batch")
        self.logger.info(f"  Loaded {len(demos)} trajectories.")
        
        obs_data, act_data = self._process_data(demos)
        self.logger.info(f"  Processed {len(obs_data)} samples.")
        
        # 2. Initialize Network
        rng = jax.random.PRNGKey(42)
        rng, init_rng = jax.random.split(rng)
        
        # Auto-detect dimensions
        obs_dim = obs_data.shape[1]
        act_dim = act_data.shape[1]
        
        network = ActorCritic(action_dim=act_dim, width=64) # Smaller net for simple task
        params = network.init(init_rng, jnp.ones((1, obs_dim)))
        
        tx = optax.adam(self.learning_rate)
        train_state = TrainState.create(
            apply_fn=network.apply,
            params=params,
            tx=tx,
        )
        
        # 3. Training Loop
        steps_per_epoch = max(1, len(obs_data) // self.batch_size)
        
        @jax.jit
        def train_step(state, batch_obs, batch_act):
            def loss_fn(params):
                # We only supervise the Actor mean output
                mean, _, _ = state.apply_fn(params, batch_obs)
                loss = jnp.mean((mean - batch_act) ** 2)
                return loss
            
            grads = jax.grad(loss_fn)(state.params)
            state = state.apply_gradients(grads=grads)
            return state, loss_fn(state.params)

        for epoch in range(self.max_epochs):
            # Shuffle
            perm = np.random.permutation(len(obs_data))
            epoch_loss = 0.0
            
            # Helper to handle small datasets
            count = 0
            for i in range(steps_per_epoch):
                # If we have less than one batch, take everything
                start = i * self.batch_size
                end = min((i + 1) * self.batch_size, len(obs_data))
                if start >= len(obs_data):
                     break # Should not happen with logic above but safe
                
                idx = perm[start:end]
                batch_obs = obs_data[idx]
                batch_act = act_data[idx]
                
                train_state, loss = train_step(train_state, batch_obs, batch_act)
                epoch_loss += loss
                count += 1
            
            avg_loss = epoch_loss / max(1, count)
            self.logger.info(f"  Epoch {epoch+1}/{self.max_epochs} | Loss: {avg_loss:.4f}")
            
            # TODO: Integrate with dashboard if needed (callback)
            
        self.logger.info("ðŸŽ’ Training complete.")
        return True, train_state

    def validate(self, model: Any) -> bool:
        self.logger.info("Validating Kindergarten model...")
        # In real impl, we should use a held-out validation set.
        # For now, we trust the training loss if it's low enough.
        # Ideally, we return True if loss < threshold.
        
        # Let's verify we output valid shapes
        try:
            dummy_obs = jnp.zeros((1, 4)) # Assuming 4-dim obs from process_data
            mean, _, _ = model.apply_fn(model.params, dummy_obs)
            valid_shape = mean.shape == (1, 2)
            self.logger.info(f"  Output shape check: {mean.shape} -> {'PASS' if valid_shape else 'FAIL'}")
            return valid_shape
        except Exception as e:
            self.logger.error(f"Validation error: {e}")
            return False

    def _process_data(self, demos: List[Dict]) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """
        Convert list of trajectories to stacked arrays.
        Flatten observations to vector.
        """
        obs_list = []
        act_list = []
        
        for traj in demos:
            for step in traj["steps"]:
                # Flatten observation: [agent_pos, goal_pos]
                # Assuming raw state dict. In reality, we need feature extraction.
                # For this simple "Grounding" station, let's just feed relative vectors.
                
                s = step["observation"]
                agent_pos = np.array(s["agent_position"])
                goal_pos = np.array(s["goal_position"])
                
                # Feature: Relative position to goal
                rel_goal = goal_pos - agent_pos
                
                # Feature: Agent angle (sin, cos)
                angle = s["agent_angle"]
                heading = np.array([np.cos(angle), np.sin(angle)])
                
                # Cat features: [rel_x, rel_y, head_x, head_y] (4 dims)
                obs_vec = np.concatenate([rel_goal, heading])
                
                # Action: Motor [v, w]
                act_vec = np.array(step["action"]["motor"])
                
                obs_list.append(obs_vec)
                act_list.append(act_vec)
        
        return jnp.array(obs_list), jnp.array(act_list)

