# ğŸ§  Entropy Engine V3 - Training Guide

Tento dokument popisuje trÃ©ninkovÃ© skripty pro Swarm Intelligence, specificky novÃ½ **Massive HOG (Hand of God)** reÅ¾im.

## ğŸš€ RychlÃ½ Start

Pro spuÅ¡tÄ›nÃ­ nejvÃ½konnÄ›jÅ¡Ã­ho trÃ©ninku (Massive Parallel + Expert Guidance):

```bash
python train_massive_hog.py
```

Tento skript automaticky:
1.  Nastartuje **64 paralelnÃ­ch prostÅ™edÃ­** (1280 agentÅ¯).
2.  Zkompiluje JAX computational graph (XLA).
3.  ZaÄne trÃ©nink s **Hand of God** asistencÃ­ (100% -> 0%).
4.  UklÃ¡dÃ¡ videa (GIF) do `outputs/massive_hog/`.

---

## ğŸ› ï¸ Konfigurace (`TrainingConfig`)

Skript `train_massive_hog.py` pouÅ¾Ã­vÃ¡ dataclass `TrainingConfig` pro nastavenÃ­ vÅ¡ech hyperparametrÅ¯.

| Parametr | Default | Popis |
| :--- | :--- | :--- |
| **Environment** | | |
| `NUM_ENVS` | `64` | PoÄet paralelnÃ­ch simulacÃ­. VyÅ¡Å¡Ã­ ÄÃ­slo = stabilnÄ›jÅ¡Ã­ gradient. |
| `NUM_AGENTS` | `20` | PoÄet agentÅ¯ v jednÃ© arÃ©nÄ›. Celkem agentÅ¯ = Envs * Agents. |
| `MAX_STEPS` | `200` | DÃ©lka jednÃ© epizody (krokÅ¯). |
| **Training** | | |
| `TOTAL_EPOCHS` | `50000` | CelkovÃ½ poÄet epoch. |
| `LR_ACTOR` | `3e-4` | Learning Rate pro Actora (pohyb). |
| `LR_CRITIC` | `1e-3` | Learning Rate pro Critica (hodnocenÃ­ stavu). |
| **Hand of God** | | |
| `HOG_START` | `1.0` | PoÄÃ¡teÄnÃ­ sÃ­la asistence (1.0 = 100% expert). |
| `HOG_END` | `0.0` | KoneÄnÃ¡ sÃ­la asistence (0.0 = ÄistÃ¡ AI). |
| `HOG_DECAY_EPOCHS` | `5000` | PoÄet epoch, bÄ›hem kterÃ½ch asistence klesne na nulu. |
| **Rendering** | | |
| `RENDER` | `True` | Zapnout/Vypnout generovÃ¡nÃ­ GIFÅ¯. |
| `RENDER_EVERY` | `1000` | Jak Äasto (v epochÃ¡ch) generovat validaÄnÃ­ video. |

---

## ğŸ”§ Jak vytvoÅ™it vlastnÃ­ experiment?

MÃ­sto editace hlavnÃ­ho souboru mÅ¯Å¾ete vytvoÅ™it vlastnÃ­ spouÅ¡tÄ›cÃ­ skript importovÃ¡nÃ­m `run_training` a `TrainingConfig`.

**PÅ™Ã­klad: `my_experiment.py`**

```python
from train_massive_hog import run_training, TrainingConfig

# 1. Definice vlastnÃ­ konfigurace
my_config = TrainingConfig(
    NUM_ENVS=16,            # MÃ©nÄ› prostÅ™edÃ­ pro debugging
    NUM_AGENTS=10, 
    TOTAL_EPOCHS=500,       # KrÃ¡tkÃ½ run
    HOG_DECAY_EPOCHS=100,   # RychlejÅ¡Ã­ Ãºstup experta
    OUTPUT_DIR="outputs/my_debug_run",
    RENDER_EVERY=50         # ÄŒastÄ›jÅ¡Ã­ videa
)

# 2. SpuÅ¡tÄ›nÃ­
if __name__ == "__main__":
    run_training(my_config)
```

---

## ğŸ§  Koncepty

### Hand of God (HOG) ğŸ‘»
TrÃ©nink zaÄÃ­nÃ¡ s "pomocnÃ½mi koleÄky".
*   **Start**: AgentÅ¯v pohyb je mixem jeho sÃ­tÄ› a "ExpertnÃ­ho Vektoru" (kterÃ½ znÃ¡ cestu k cÃ­li).
*   **PrÅ¯bÄ›h**: PomÄ›r experta lineÃ¡rnÄ› klesÃ¡. SÃ­Å¥ se uÄÃ­ predikovat to, co by udÄ›lal expert (Imitation Learning via PPO Rewards).
*   **Konec**: Expert zmizÃ­ a agent se pohybuje zcela samostatnÄ›.

### Massive Parallelism (JAX VMAP) ğŸŒ
MÃ­sto jednÃ© simulace bÄ›Å¾Ã­ 64 simulacÃ­ narÃ¡z na jednÃ© grafickÃ© kartÄ› (nebo CPU via AVX).
*   **VÃ½hoda**: ObrovskÃ© mnoÅ¾stvÃ­ dat (Experience Replay) za zlomek Äasu.
*   **DÅ¯sledek**: ExtrÃ©mnÄ› stabilnÃ­ uÄenÃ­, protoÅ¾e `mean_reward` je prÅ¯mÄ›rovÃ¡n pÅ™es 1280 agentÅ¯, nikoliv jen 20.
