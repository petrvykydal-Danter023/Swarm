{
    "task_name": "shift_optimization",
    "description": "Optimize work shift assignments using grid-based swarm agents",
    "observation_type": "grid",
    "action_space_type": "discrete",
    "env_params": {
        "world_width": 24,
        "world_height": 7,
        "num_agents": 5,
        "physics": {
            "gravity_y": 0.0,
            "friction": 0.0,
            "time_step": 1.0
        },
        "special_objects": [
            {
                "type": "goal",
                "x": 8,
                "y": 0
            },
            {
                "type": "goal",
                "x": 8,
                "y": 1
            },
            {
                "type": "goal",
                "x": 8,
                "y": 2
            },
            {
                "type": "goal",
                "x": 12,
                "y": 3
            },
            {
                "type": "goal",
                "x": 12,
                "y": 4
            },
            {
                "type": "goal",
                "x": 16,
                "y": 5
            },
            {
                "type": "goal",
                "x": 16,
                "y": 6
            }
        ]
    },
    "reward_code": "# Shift optimization reward\n# Grid represents: X = time slots (hours), Y = employees\n# Goals represent required coverage slots\n\nreward = 0.0\n\n# Reward for being at a goal position (required shift)\nfor goal in env_state['goals']:\n    if abs(agent['x'] - goal['x']) < 1 and abs(agent['y'] - goal['y']) < 1:\n        reward += 5.0\n        break\n\n# Penalty for overlapping with other agents (double coverage)\nfor neighbor in env_state['neighbors']:\n    if abs(agent['x'] - neighbor['x']) < 1 and abs(agent['y'] - neighbor['y']) < 1:\n        reward -= 2.0\n\n# Small reward for being within work hours (8-18)\nif 8 <= agent['x'] <= 18:\n    reward += 0.1",
    "training_params": {
        "algo": "PPO",
        "total_timesteps": 30000,
        "learning_rate": 0.0005,
        "gamma": 0.95,
        "batch_size": 32,
        "n_envs": 4,
        "max_episode_steps": 100
    }
}