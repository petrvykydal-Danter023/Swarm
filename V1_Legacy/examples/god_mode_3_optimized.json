{
    "task_name": "god_mode_3_optimized",
    "world_width": 100.0,
    "world_height": 100.0,
    "env_params": {
        "num_agents": 5,
        "dt": 0.1,
        "friction": 0.2,
        "goal_type": "dynamic",
        "agent_mode": "nav",
        "special_objects": [
            {
                "type": "goal",
                "x": 50,
                "y": 50,
                "radius": 5
            }
        ],
        "sensors": [
            "position",
            "velocity",
            "goal_vector",
            "obstacle_radar"
        ],
        "reward_code": "reward = 0.0\n\n# 1. Goal + Alignment\ngoal = env_state['goals'][0]\ndx, dy = goal['x'] - agent['x'], goal['y'] - agent['y']\ndist = math.sqrt(dx**2 + dy**2)\nreward -= dist / 50.0\nif dist < goal.get('radius', 5.0): reward += 20.0\nif dist > 0.001:\n    nx, ny = dx / dist, dy / dist\n    reward += (agent['vx'] * nx + agent['vy'] * ny) * 0.5\n\n# 2. TTC (GOD MODE PHASE 2)\n# Predict collision with walls\nw, h = env_state.get('world_width', 100), env_state.get('world_height', 100)\nvx, vy = agent['vx'], agent['vy']\nspeed = math.sqrt(vx**2 + vy**2)\n# Wall Prediction (Simple)\nmargin = 2.0\nttc = 999.0\nif vx > 0.1: ttc = min(ttc, (w - agent['x'] - margin) / vx)\nif vx < -0.1: ttc = min(ttc, (agent['x'] - margin) / abs(vx))\nif vy > 0.1: ttc = min(ttc, (h - agent['y'] - margin) / vy)\nif vy < -0.1: ttc = min(ttc, (agent['y'] - margin) / abs(vy))\n\n# Obstacle Prediction (Circle-Circle moving)\n# For simplicity in reward string, assumed static\nfor obs in env_state.get('obstacles', []):\n    # Relative pos/vel\n    rx, ry = agent['x'] - obs['x'], agent['y'] - obs['y']\n    rvx, rvy = vx, vy\n    # Quadratic: |P + V*t|^2 = (r_agent + r_obs)^2\n    # a*t^2 + b*t + c = 0\n    r_sum = agent.get('radius', 2.0) + obs.get('radius', 5.0) + 1.0\n    a = rvx**2 + rvy**2\n    if a > 0.01:\n        b = 2 * (rx*rvx + ry*rvy)\n        c = (rx**2 + ry**2) - r_sum**2\n        delta = b**2 - 4*a*c\n        if delta >= 0:\n            t1 = (-b - math.sqrt(delta)) / (2*a)\n            if 0 < t1 < ttc: ttc = t1\n\n# Apply TTC Penalty\nif ttc < 1.0:\n    reward -= 10.0 * (1.0 - ttc)\n\n# 3. Smoothness\nreward -= speed * 0.05\nif 'last_action' in agent and 'action' in agent:\n    diff = agent['action'] - agent['last_action']\n    jerk = np.sum(np.abs(diff))\n    reward -= jerk * 0.1"
    },
    "training_params": {
        "algo": "PPO",
        "total_timesteps": 50000,
        "learning_rate": 0.0003,
        "max_episode_steps": 300,
        "n_envs": 4,
        "save_model_path": "models/saved/god_mode_3"
    }
}