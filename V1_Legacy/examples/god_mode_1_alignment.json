{
    "task_name": "god_mode_1_alignment",
    "world_width": 100.0,
    "world_height": 100.0,
    "env_params": {
        "num_agents": 5,
        "dt": 0.1,
        "friction": 0.2,
        "goal_type": "dynamic",
        "agent_mode": "nav",
        "special_objects": [
            {
                "type": "goal",
                "x": 50,
                "y": 50,
                "radius": 5
            }
        ],
        "sensors": [
            "position",
            "velocity",
            "goal_vector",
            "obstacle_radar"
        ],
        "reward_code": "reward = 0.0\n\n# 1. Goal Distance\ngoal = env_state['goals'][0]\ndx, dy = goal['x'] - agent['x'], goal['y'] - agent['y']\ndist = math.sqrt(dx**2 + dy**2)\nreward -= dist / 50.0\nif dist < goal.get('radius', 5.0): reward += 20.0\n\n# 2. Velocity Alignment (GOD MODE PHASE 1)\n# Normalize goal direction\nif dist > 0.001:\n    nx, ny = dx / dist, dy / dist\nelse:\n    nx, ny = 0, 0\n# Project velocity onto direction\nalignment = agent['vx'] * nx + agent['vy'] * ny\n# Reward for moving towards goal (positive dot product)\nreward += alignment * 0.5\n\n# 3. Wall Penalty (Standard)\nw, h = env_state.get('world_width', 100), env_state.get('world_height', 100)\nmargin = 5.0\nif agent['x'] < margin or agent['x'] > w - margin or agent['y'] < margin or agent['y'] > h - margin:\n    reward -= 2.0\n    if agent['x'] < 2.0 or agent['x'] > w - 2.0 or agent['y'] < 2.0 or agent['y'] > h - 2.0: reward -= 5.0\n\n# 4. Obstacles\nmin_dist = 999.0\nfor obs in env_state.get('obstacles', []):\n    d = math.sqrt((agent['x']-obs['x'])**2 + (agent['y']-obs['y'])**2) - obs.get('radius', 5)\n    min_dist = min(min_dist, d)\nif min_dist < 4.0:\n    reward -= (4.0 - min_dist) * 1.0\n\n# 5. Smoothness\nspeed = math.sqrt(agent['vx']**2 + agent['vy']**2)\nreward -= speed * 0.05\nif 'last_action' in agent and 'action' in agent:\n    diff = agent['action'] - agent['last_action']\n    jerk = np.sum(np.abs(diff))\n    reward -= jerk * 0.1"
    },
    "training_params": {
        "algo": "PPO",
        "total_timesteps": 50000,
        "learning_rate": 0.0003,
        "max_episode_steps": 300,
        "n_envs": 1,
        "save_model_path": "models/saved/god_mode_1"
    }
}