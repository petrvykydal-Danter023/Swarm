{
    "task_name": "dynamic_goal_demo",
    "world_width": 100.0,
    "world_height": 100.0,
    "env_params": {
        "num_agents": 5,
        "dt": 0.1,
        "friction": 0.2,
        "goal_type": "dynamic",
        "agent_mode": "nav",
        "special_objects": [
            {
                "type": "goal",
                "x": 50,
                "y": 50,
                "radius": 5
            }
        ],
        "sensors": [
            "position",
            "velocity",
            "goal_vector",
            "obstacle_radar",
            "energy"
        ],
        "reward_code": "reward = 0.0\n\n# 1. Goal\ngoal = env_state['goals'][0]\ndist = math.sqrt((agent['x']-goal['x'])**2 + (agent['y']-goal['y'])**2)\nreward -= dist / 50.0\nif dist < goal.get('radius', 5.0):\n    reward += 20.0\n\n# 2. Wall Penalty\nw, h = env_state.get('world_width', 100), env_state.get('world_height', 100)\nmargin = 5.0\nif agent['x'] < margin or agent['x'] > w - margin or agent['y'] < margin or agent['y'] > h - margin:\n    reward -= 2.0\n    if agent['x'] < 2.0 or agent['x'] > w - 2.0 or agent['y'] < 2.0 or agent['y'] > h - 2.0: reward -= 5.0\n\n# 3. Obstacles\nmin_dist = 999.0\nfor obs in env_state.get('obstacles', []):\n    d = math.sqrt((agent['x']-obs['x'])**2 + (agent['y']-obs['y'])**2) - obs.get('radius', 5)\n    min_dist = min(min_dist, d)\nif min_dist < 4.0:\n    reward -= (4.0 - min_dist) * 1.0\n\n# 4. Smoothness\nspeed = math.sqrt(agent['vx']**2 + agent['vy']**2)\nreward -= speed * 0.05\nif 'last_action' in agent and 'action' in agent:\n    diff = agent['action'] - agent['last_action']\n    jerk = np.sum(np.abs(diff))\n    reward -= jerk * 0.1"
    },
    "training_params": {
        "algo": "PPO",
        "total_timesteps": 50000,
        "learning_rate": 0.0003,
        "max_episode_steps": 300,
        "n_envs": 1,
        "save_model_path": "models/saved/dynamic_goal_model"
    }
}