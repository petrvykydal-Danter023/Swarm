# Sim2Real Reward Function Design

Pro ÃºspÄ›Å¡nÃ½ pÅ™enos nÃ¡trenovanÃ©ho chovÃ¡nÃ­ do reality (a 3D svÄ›ta) nesmÃ­ bÃ½t Reward Function pouze o "splnÄ›nÃ­ Ãºkolu". MusÃ­ aktivnÄ› tvarovat chovÃ¡nÃ­ tak, aby bylo proveditelnÃ© na fyzickÃ©m hardwaru.

## 1. Safety & Hardware Protection (Ochrana HW)

Roboti nejsou nezniÄitelnÃ­. V simulaci mohou do sebe narÃ¡Å¾et v plnÃ© rychlosti, v realitÄ› to znamenÃ¡ zniÄenÃ© motory a plasty.

### âŒ Current State
```python
reward = -distance_to_goal
# Agent se snaÅ¾Ã­ dostat k cÃ­li za kaÅ¾dou cenu, i kdyÅ¾ to znamenÃ¡ nÃ¡raz v plnÃ© rychlosti.
```

### âœ… Proposed Upgrade: `collision_penalty` & `velocity_cap`
```python
# 1. High Velocity Collision Penalty
# Pokud dojde ke kolizi a rychlost byla vysokÃ¡ -> velkÃ½ trest.
min_safe_dist = agent.radius + 2.0
nearest_dist = 999
for other in env_state['agents']:
    if other['id'] != agent['id']:
        d = dist(agent, other)
        nearest_dist = min(nearest_dist, d)

if nearest_dist < min_safe_dist:
    # Penalizovat rychlost v blÃ­zkosti pÅ™ekÃ¡Å¾ek
    speed = math.sqrt(agent['vx']**2 + agent['vy']**2)
    if speed > 0.5: # 50% speed limit near obstacles
        reward -= speed * 2.0 
```

## 2. Energy Efficiency (SpotÅ™eba Energie)

ReÃ¡lnÃ­ roboti majÃ­ omezenou baterii. Agenti se musÃ­ nauÄit "Å¡etÅ™it", ne jen "sprintovat".

### âŒ Current State
```python
# Å½Ã¡dnÃ¡ penalizace za pohyb. Agent kmitÃ¡ sem a tam.
```

### âœ… Proposed Upgrade: `energy_cost` & `idle_reward`
```python
# 1. Action Magnitude Penalty (JemnÄ›jÅ¡Ã­ pohyby)
# reward -= (abs(ax) + abs(ay)) * 0.01

# 2. Battery Awareness
# Pokud mÃ¡lo baterie -> VÄ›tÅ¡Ã­ motivace nic nedÄ›lat (Å¡etÅ™it).
if agent['energy'] < 0.2:
    if abs(agent['vx']) < 0.01:
        reward += 0.1 # OdmÄ›na za odpoÄinek pÅ™i vybitÃ­
```

## 3. Smoothness Control (Ochrana pÅ™evodovek/MotorÅ¯)

PrudkÃ© zmÄ›ny smÄ›ru (Jerk) niÄÃ­ pÅ™evodovky a zpÅ¯sobujÃ­ prokluz kol.

### âŒ Current State
```python
# Agent mÅ¯Å¾e mÄ›nit smÄ›r okamÅ¾itÄ› (pokud to fyzika dovolÃ­).
```

### âœ… Proposed Upgrade: `action_smoothing`
```python
# VyÅ¾aduje historii akcÃ­ (kterou mÃ¡me v Motor Lag bufferu!)
# reward -= abs(current_action - last_action) * 0.5
# NutÃ­ agenta mÄ›nit akce plynule.
```

## 4. Communication Efficiency (Bandwidth)

VysÃ­lÃ¡nÃ­ zprÃ¡v stojÃ­ energii a zahlcuje sÃ­Å¥.

### âŒ Current State
```python
# Agent mÅ¯Å¾e "kÅ™iÄet" (signal=1.0) neustÃ¡le bez trestu (kromÄ› malÃ© energy cost v enginu).
```

### âœ… Proposed Upgrade: `silence_reward`
```python
# OdmÄ›na za mlÄenÃ­, pokud zprÃ¡va nenÃ­ nutnÃ¡.
if abs(agent['comm_signal']) < 0.1:
    reward += 0.05
# TÃ­m se nauÄÃ­ komunikovat jen kdyÅ¾ je to dÅ¯leÅ¾itÃ©.
```

## 5. Sparse vs Dense Rewards (TrÃ©ninkovÃ¡ strategie)

*   **Dense (HustÃ©)**: `reward = -distance`. NavÃ¡dÃ­ agenta krok po kroku. RychlÃ© uÄenÃ­, ale nÃ¡chylnÃ© na lokÃ¡lnÃ­ minima (zasekne se za zdÃ­).
*   **Sparse (Å˜Ã­dkÃ©)**: `reward = 100 if reached_goal else 0`. TÄ›Å¾kÃ© na nauÄenÃ­ (agent bloudÃ­), ale robustnÄ›jÅ¡Ã­ strategie.

### ğŸ’¡ HybridnÃ­ pÅ™Ã­stup (Curriculum Learning)
1.  ZaÄÃ­t s **Dense** odmÄ›nami (aby pochopil, co mÃ¡ dÄ›lat).
2.  PostupnÄ› ($alpha \to 0$) pÅ™ejÃ­t na **Sparse** (aby naÅ¡el nejlepÅ¡Ã­ cestu, ne jen nÃ¡sledoval gradient vzdÃ¡lenosti).

## PÅ™Ã­klad komplexnÃ­ "Sim2Real" funkce (Python Code String)

```python
reward = 0.0

# --- 1. Objective (The Goal) ---
goal = env_state['goals'][0]
dist = math.sqrt((agent['x']-goal['x'])**2 + (agent['y']-goal['y'])**2)
reward += -dist / 100.0 # Dense guidance
if dist < 5.0: reward += 50.0 # Success bonus

# --- 2. Safety (Collision Avoidance) ---
# Detect near obstacles using radar or positions
min_dist = 999
for o in env_state.get('obstacles', []): # Assuming obstacle list available in env_state or radar
    d = math.sqrt((agent['x']-o['x'])**2 + (agent['y']-o['y'])**2) - o['radius'] - agent['radius']
    min_dist = min(min_dist, d)

if min_dist < 2.0:
    reward -= 1.0 # Proximity warning
    if min_dist <= 0:
        reward -= 10.0 # Collision!

# --- 3. Efficiency (Energy & Smoothness) ---
speed = math.sqrt(agent['vx']**2 + agent['vy']**2)
accel = math.sqrt(agent.get('ax', 0)**2 + agent.get('ay', 0)**2) # Need to pass last action to env_state

reward -= speed * 0.01 # Cost of transport
reward -= accel * 0.05 # Cost of acceleration (Jerk/Motor load)

# --- 4. Comms ---
if abs(agent.get('comm', 0)) > 0.1:
    reward -= 0.05 # Talking costs bandwidth

return float(reward)
```
